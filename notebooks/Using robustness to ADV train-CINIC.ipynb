{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = '/tmp/'\n",
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ADV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashraf.haddad/.conda/envs/rob/lib/python3.9/site-packages/robustness/train.py:24: UserWarning: Could not import amp.\n",
      "  warnings.warn('Could not import amp.')\n"
     ]
    }
   ],
   "source": [
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.datasets import CINIC#CIFAR#FashionMnist\n",
    "from robustness import data_augmentation as da\n",
    "import torch \n",
    "import torchvision.datasets\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('/home/u21010246/mlpr/venv/lib/python3.8/site-packages/robustness')\n",
    "\n",
    "# We use cox (http://github.com/MadryLab/cox) to log, store and analyze\n",
    "# results. Read more at https//cox.readthedocs.io.\n",
    "from cox.utils import Parameters\n",
    "import cox.store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cinic..\n"
     ]
    }
   ],
   "source": [
    "# Hard-coded dataset, architecture, batch size, workers\n",
    "ds = CINIC('/tmp/cinic') # CIFAR('/tmp')\n",
    "m, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds)  #same model as std training \n",
    "train_loader, val_loader = ds.make_loaders(batch_size=BATCH_SIZE, workers=NUM_WORKERS)\n",
    "\n",
    "#wget http://data.csail.mit.edu/places/places365/places365standard_easyformat.tar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a cox store for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /tmp/db5d66ec-222a-4f7b-aa2b-86da20d95cf9\n"
     ]
    }
   ],
   "source": [
    "# Create a cox store for logging  , later useful for tensboard to visualize the accuracy curve over epochs\n",
    "out_store = cox.store.Store(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded base parameters\n",
    "train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'adv_train': 1,\n",
    "    'constraint': '2',\n",
    "    'eps': 0.5,\n",
    "    'attack_lr': 0.1,\n",
    "    'attack_steps': 7,\n",
    "    'epochs': 200\n",
    "}\n",
    "train_args = Parameters(train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.TRAINING_ARGS, CINIC)\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.PGD_ARGS, CINIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 | Loss 2.5480 | AdvPrec1 15.230 | AdvPrec5 59.977 | Reg term: 0.0 \n",
      "Val Epoch:0 | Loss 2.1089 | NatPrec1 21.974 | NatPrec5 74.239 | Reg term: 0.0 ||\n",
      "Val Epoch:0 | Loss 2.1795 | AdvPrec1 18.839 | AdvPrec5 67.956 | Reg term: 0.0 ||\n",
      "Train Epoch:1 | Loss 2.1525 | AdvPrec1 18.893 | AdvPrec5 69.399 | Reg term: 0.0 \n",
      "Train Epoch:2 | Loss 2.0387 | AdvPrec1 22.643 | AdvPrec5 76.241 | Reg term: 0.0 \n",
      "Train Epoch:3 | Loss 1.9546 | AdvPrec1 25.924 | AdvPrec5 80.148 | Reg term: 0.0 \n",
      "Train Epoch:4 | Loss 1.8887 | AdvPrec1 28.414 | AdvPrec5 82.617 | Reg term: 0.0 \n",
      "Train Epoch:5 | Loss 1.8327 | AdvPrec1 30.650 | AdvPrec5 84.324 | Reg term: 0.0 \n",
      "Val Epoch:5 | Loss 1.5644 | NatPrec1 42.577 | NatPrec5 90.860 | Reg term: 0.0 ||\n",
      "Val Epoch:5 | Loss 1.8153 | AdvPrec1 30.802 | AdvPrec5 85.190 | Reg term: 0.0 ||\n",
      "Train Epoch:6 | Loss 1.7809 | AdvPrec1 32.760 | AdvPrec5 85.717 | Reg term: 0.0 \n",
      "Train Epoch:7 | Loss 1.7405 | AdvPrec1 34.258 | AdvPrec5 86.703 | Reg term: 0.0 \n",
      "Train Epoch:8 | Loss 1.7102 | AdvPrec1 35.119 | AdvPrec5 87.432 | Reg term: 0.0 \n",
      "Train Epoch:9 | Loss 1.6824 | AdvPrec1 36.372 | AdvPrec5 88.066 | Reg term: 0.0 \n",
      "Train Epoch:10 | Loss 1.6594 | AdvPrec1 37.284 | AdvPrec5 88.531 | Reg term: 0.0\n",
      "Val Epoch:10 | Loss 1.3273 | NatPrec1 51.520 | NatPrec5 93.620 | Reg term: 0.0 |\n",
      "Val Epoch:10 | Loss 1.6762 | AdvPrec1 37.293 | AdvPrec5 88.458 | Reg term: 0.0 |\n",
      "Train Epoch:11 | Loss 1.6412 | AdvPrec1 37.781 | AdvPrec5 88.890 | Reg term: 0.0\n",
      "Train Epoch:12 | Loss 1.6201 | AdvPrec1 38.940 | AdvPrec5 89.411 | Reg term: 0.0\n",
      "Train Epoch:13 | Loss 1.6095 | AdvPrec1 39.358 | AdvPrec5 89.400 | Reg term: 0.0\n",
      "Train Epoch:14 | Loss 1.5966 | AdvPrec1 39.713 | AdvPrec5 89.787 | Reg term: 0.0\n",
      "Train Epoch:15 | Loss 1.5859 | AdvPrec1 40.050 | AdvPrec5 89.877 | Reg term: 0.0\n",
      "Val Epoch:15 | Loss 1.2521 | NatPrec1 54.319 | NatPrec5 94.302 | Reg term: 0.0 |\n",
      "Val Epoch:15 | Loss 1.6730 | AdvPrec1 38.242 | AdvPrec5 89.319 | Reg term: 0.0 |\n",
      "Train Epoch:16 | Loss 1.5757 | AdvPrec1 40.708 | AdvPrec5 90.160 | Reg term: 0.0\n",
      "Train Epoch:17 | Loss 1.5685 | AdvPrec1 41.002 | AdvPrec5 90.270 | Reg term: 0.0\n",
      "Train Epoch:18 | Loss 1.5572 | AdvPrec1 41.417 | AdvPrec5 90.439 | Reg term: 0.0\n",
      "Train Epoch:19 | Loss 1.5524 | AdvPrec1 41.512 | AdvPrec5 90.644 | Reg term: 0.0\n",
      "Train Epoch:20 | Loss 1.5454 | AdvPrec1 41.746 | AdvPrec5 90.472 | Reg term: 0.0\n",
      "Val Epoch:20 | Loss 1.1925 | NatPrec1 56.711 | NatPrec5 95.038 | Reg term: 0.0 |\n",
      "Val Epoch:20 | Loss 1.6026 | AdvPrec1 39.707 | AdvPrec5 89.951 | Reg term: 0.0 |\n",
      "Train Epoch:21 | Loss 1.5415 | AdvPrec1 41.981 | AdvPrec5 90.556 | Reg term: 0.0\n",
      "Train Epoch:22 | Loss 1.5330 | AdvPrec1 42.313 | AdvPrec5 90.800 | Reg term: 0.0\n",
      "Train Epoch:23 | Loss 1.5254 | AdvPrec1 42.647 | AdvPrec5 90.782 | Reg term: 0.0\n",
      "Train Epoch:24 | Loss 1.5252 | AdvPrec1 42.662 | AdvPrec5 90.884 | Reg term: 0.0\n",
      "Train Epoch:25 | Loss 1.5213 | AdvPrec1 42.790 | AdvPrec5 90.961 | Reg term: 0.0\n",
      "Val Epoch:25 | Loss 1.1658 | NatPrec1 58.132 | NatPrec5 95.679 | Reg term: 0.0 |\n",
      "Val Epoch:25 | Loss 1.5990 | AdvPrec1 39.370 | AdvPrec5 90.998 | Reg term: 0.0 |\n",
      "Train Epoch:26 | Loss 1.5188 | AdvPrec1 42.986 | AdvPrec5 91.054 | Reg term: 0.0\n",
      "Train Epoch:27 | Loss 1.5159 | AdvPrec1 43.033 | AdvPrec5 91.074 | Reg term: 0.0\n",
      "Train Epoch:28 | Loss 1.5098 | AdvPrec1 43.268 | AdvPrec5 91.059 | Reg term: 0.0\n",
      "Train Epoch:29 | Loss 1.5060 | AdvPrec1 43.373 | AdvPrec5 91.214 | Reg term: 0.0\n",
      "Train Epoch:30 | Loss 1.5063 | AdvPrec1 43.526 | AdvPrec5 91.198 | Reg term: 0.0\n",
      "Val Epoch:30 | Loss 1.1459 | NatPrec1 60.788 | NatPrec5 95.261 | Reg term: 0.0 |\n",
      "Val Epoch:30 | Loss 1.5771 | AdvPrec1 42.153 | AdvPrec5 89.740 | Reg term: 0.0 |\n",
      "Train Epoch:31 | Loss 1.5021 | AdvPrec1 43.530 | AdvPrec5 91.356 | Reg term: 0.0\n",
      "Train Epoch:32 | Loss 1.5000 | AdvPrec1 43.721 | AdvPrec5 91.262 | Reg term: 0.0\n",
      "Train Epoch:33 | Loss 1.4974 | AdvPrec1 43.639 | AdvPrec5 91.320 | Reg term: 0.0\n",
      "Train Epoch:34 | Loss 1.4943 | AdvPrec1 43.983 | AdvPrec5 91.370 | Reg term: 0.0\n",
      "Train Epoch:35 | Loss 1.4890 | AdvPrec1 44.136 | AdvPrec5 91.504 | Reg term: 0.0\n",
      "Val Epoch:35 | Loss 1.1763 | NatPrec1 57.359 | NatPrec5 95.540 | Reg term: 0.0 |\n",
      "Val Epoch:35 | Loss 1.6056 | AdvPrec1 39.870 | AdvPrec5 90.676 | Reg term: 0.0 |\n",
      "Train Epoch:36 | Loss 1.4860 | AdvPrec1 44.012 | AdvPrec5 91.518 | Reg term: 0.0\n",
      "Train Epoch:37 | Loss 1.4867 | AdvPrec1 44.227 | AdvPrec5 91.396 | Reg term: 0.0\n",
      "Train Epoch:38 | Loss 1.4841 | AdvPrec1 44.316 | AdvPrec5 91.574 | Reg term: 0.0\n",
      "Train Epoch:39 | Loss 1.4827 | AdvPrec1 44.423 | AdvPrec5 91.536 | Reg term: 0.0\n",
      "Train Epoch:40 | Loss 1.4861 | AdvPrec1 44.064 | AdvPrec5 91.520 | Reg term: 0.0\n",
      "Val Epoch:40 | Loss 1.0807 | NatPrec1 60.820 | NatPrec5 96.160 | Reg term: 0.0 |\n",
      "Val Epoch:40 | Loss 1.5302 | AdvPrec1 42.374 | AdvPrec5 91.952 | Reg term: 0.0 |\n",
      "Train Epoch:41 | Loss 1.4771 | AdvPrec1 44.582 | AdvPrec5 91.588 | Reg term: 0.0\n",
      "Train Epoch:42 | Loss 1.4769 | AdvPrec1 44.556 | AdvPrec5 91.549 | Reg term: 0.0\n",
      "Train Epoch:43 | Loss 1.4750 | AdvPrec1 44.661 | AdvPrec5 91.708 | Reg term: 0.0\n",
      "Train Epoch:44 | Loss 1.4724 | AdvPrec1 44.760 | AdvPrec5 91.574 | Reg term: 0.0\n",
      "Train Epoch:45 | Loss 1.4725 | AdvPrec1 44.612 | AdvPrec5 91.690 | Reg term: 0.0\n",
      "Val Epoch:45 | Loss 1.0743 | NatPrec1 62.616 | NatPrec5 96.118 | Reg term: 0.0 |\n",
      "Val Epoch:45 | Loss 1.4939 | AdvPrec1 43.966 | AdvPrec5 91.596 | Reg term: 0.0 |\n",
      "Train Epoch:46 | Loss 1.4692 | AdvPrec1 44.904 | AdvPrec5 91.736 | Reg term: 0.0\n",
      "Train Epoch:47 | Loss 1.4691 | AdvPrec1 44.939 | AdvPrec5 91.778 | Reg term: 0.0\n",
      "Train Epoch:48 | Loss 1.4709 | AdvPrec1 44.848 | AdvPrec5 91.651 | Reg term: 0.0\n",
      "Train Epoch:49 | Loss 1.4673 | AdvPrec1 44.881 | AdvPrec5 91.759 | Reg term: 0.0\n",
      "Train Epoch:50 | Loss 1.3066 | AdvPrec1 50.836 | AdvPrec5 93.776 | Reg term: 0.0\n",
      "Val Epoch:50 | Loss 0.8371 | NatPrec1 71.340 | NatPrec5 97.670 | Reg term: 0.0 |\n",
      "Val Epoch:50 | Loss 1.2648 | AdvPrec1 52.334 | AdvPrec5 94.492 | Reg term: 0.0 |\n",
      "Train Epoch:51 | Loss 1.2534 | AdvPrec1 52.727 | AdvPrec5 94.402 | Reg term: 0.0\n",
      "Train Epoch:52 | Loss 1.2329 | AdvPrec1 53.321 | AdvPrec5 94.726 | Reg term: 0.0\n",
      "Train Epoch:53 | Loss 1.2155 | AdvPrec1 54.086 | AdvPrec5 94.866 | Reg term: 0.0\n",
      "Train Epoch:54 | Loss 1.2029 | AdvPrec1 54.642 | AdvPrec5 94.999 | Reg term: 0.0\n",
      "Train Epoch:55 | Loss 1.1938 | AdvPrec1 54.979 | AdvPrec5 95.163 | Reg term: 0.0\n",
      "Val Epoch:55 | Loss 0.7777 | NatPrec1 73.068 | NatPrec5 97.936 | Reg term: 0.0 |\n",
      "Val Epoch:55 | Loss 1.2401 | AdvPrec1 53.357 | AdvPrec5 94.876 | Reg term: 0.0 |\n",
      "Train Epoch:56 | Loss 1.1837 | AdvPrec1 55.192 | AdvPrec5 95.139 | Reg term: 0.0\n",
      "Train Epoch:57 | Loss 1.1717 | AdvPrec1 55.584 | AdvPrec5 95.331 | Reg term: 0.0\n",
      "Train Epoch:58 | Loss 1.1675 | AdvPrec1 55.953 | AdvPrec5 95.449 | Reg term: 0.0\n",
      "Train Epoch:59 | Loss 1.1621 | AdvPrec1 55.884 | AdvPrec5 95.503 | Reg term: 0.0\n",
      "Train Epoch:60 | Loss 1.1534 | AdvPrec1 56.276 | AdvPrec5 95.560 | Reg term: 0.0\n",
      "Val Epoch:60 | Loss 0.7596 | NatPrec1 73.741 | NatPrec5 98.010 | Reg term: 0.0 |\n",
      "Val Epoch:60 | Loss 1.2632 | AdvPrec1 52.989 | AdvPrec5 94.761 | Reg term: 0.0 |\n",
      "Train Epoch:61 | Loss 1.1482 | AdvPrec1 56.472 | AdvPrec5 95.630 | Reg term: 0.0\n",
      "Train Epoch:62 | Loss 1.1437 | AdvPrec1 56.682 | AdvPrec5 95.697 | Reg term: 0.0\n",
      "Train Epoch:63 | Loss 1.1329 | AdvPrec1 56.850 | AdvPrec5 95.893 | Reg term: 0.0\n",
      "Train Epoch:64 | Loss 1.1309 | AdvPrec1 57.063 | AdvPrec5 95.804 | Reg term: 0.0\n",
      "Train Epoch:65 | Loss 1.1249 | AdvPrec1 57.128 | AdvPrec5 95.939 | Reg term: 0.0\n",
      "Val Epoch:65 | Loss 0.7668 | NatPrec1 73.213 | NatPrec5 98.022 | Reg term: 0.0 |\n",
      "Val Epoch:65 | Loss 1.2718 | AdvPrec1 52.347 | AdvPrec5 94.653 | Reg term: 0.0 |\n",
      "Train Epoch:66 | Loss 1.1170 | AdvPrec1 57.397 | AdvPrec5 96.096 | Reg term: 0.0\n",
      "Train Epoch:67 | Loss 1.1144 | AdvPrec1 57.491 | AdvPrec5 96.083 | Reg term: 0.0\n",
      "Train Epoch:68 | Loss 1.1072 | AdvPrec1 57.751 | AdvPrec5 96.103 | Reg term: 0.0\n",
      "Train Epoch:69 | Loss 1.1004 | AdvPrec1 57.846 | AdvPrec5 96.190 | Reg term: 0.0\n",
      "Train Epoch:70 | Loss 1.0956 | AdvPrec1 58.104 | AdvPrec5 96.267 | Reg term: 0.0\n",
      "Val Epoch:70 | Loss 0.7836 | NatPrec1 72.982 | NatPrec5 97.861 | Reg term: 0.0 |\n",
      "Val Epoch:70 | Loss 1.3350 | AdvPrec1 51.123 | AdvPrec5 94.196 | Reg term: 0.0 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:71 | Loss 1.0879 | AdvPrec1 58.301 | AdvPrec5 96.380 | Reg term: 0.0\n",
      "Train Epoch:72 | Loss 1.0806 | AdvPrec1 58.408 | AdvPrec5 96.447 | Reg term: 0.0\n",
      "Train Epoch:73 | Loss 1.0790 | AdvPrec1 58.540 | AdvPrec5 96.399 | Reg term: 0.0\n",
      "Train Epoch:74 | Loss 1.0703 | AdvPrec1 59.040 | AdvPrec5 96.478 | Reg term: 0.0\n",
      "Train Epoch:75 | Loss 1.0652 | AdvPrec1 59.053 | AdvPrec5 96.520 | Reg term: 0.0\n",
      "Val Epoch:75 | Loss 0.7560 | NatPrec1 73.508 | NatPrec5 97.918 | Reg term: 0.0 |\n",
      "Val Epoch:75 | Loss 1.3188 | AdvPrec1 52.053 | AdvPrec5 94.373 | Reg term: 0.0 |\n",
      "Train Epoch:76 | Loss 1.0618 | AdvPrec1 59.153 | AdvPrec5 96.631 | Reg term: 0.0\n",
      "Train Epoch:77 | Loss 1.0490 | AdvPrec1 59.646 | AdvPrec5 96.757 | Reg term: 0.0\n",
      "Train Epoch:78 | Loss 1.0460 | AdvPrec1 59.734 | AdvPrec5 96.732 | Reg term: 0.0\n",
      "Train Epoch:79 | Loss 1.0385 | AdvPrec1 59.988 | AdvPrec5 96.891 | Reg term: 0.0\n",
      "Train Epoch:80 | Loss 1.0337 | AdvPrec1 60.076 | AdvPrec5 96.942 | Reg term: 0.0\n",
      "Val Epoch:80 | Loss 0.7579 | NatPrec1 73.906 | NatPrec5 98.009 | Reg term: 0.0 |\n",
      "Val Epoch:80 | Loss 1.3573 | AdvPrec1 50.644 | AdvPrec5 94.191 | Reg term: 0.0 |\n",
      "Train Epoch:81 | Loss 1.0273 | AdvPrec1 60.220 | AdvPrec5 96.947 | Reg term: 0.0\n",
      "Train Epoch:82 | Loss 1.0212 | AdvPrec1 60.468 | AdvPrec5 97.097 | Reg term: 0.0\n",
      "Train Epoch:83 | Loss 1.0152 | AdvPrec1 60.612 | AdvPrec5 97.089 | Reg term: 0.0\n",
      "Train Epoch:84 | Loss 1.0085 | AdvPrec1 60.850 | AdvPrec5 97.164 | Reg term: 0.0\n",
      "Train Epoch:85 | Loss 1.0076 | AdvPrec1 60.944 | AdvPrec5 97.186 | Reg term: 0.0\n",
      "Val Epoch:85 | Loss 0.7346 | NatPrec1 74.607 | NatPrec5 97.967 | Reg term: 0.0 |\n",
      "Val Epoch:85 | Loss 1.4084 | AdvPrec1 50.589 | AdvPrec5 94.087 | Reg term: 0.0 |\n",
      "Train Epoch:86 | Loss 0.9987 | AdvPrec1 61.127 | AdvPrec5 97.260 | Reg term: 0.0\n",
      "Train Epoch:87 | Loss 0.9936 | AdvPrec1 61.494 | AdvPrec5 97.301 | Reg term: 0.0\n",
      "Train Epoch:88 | Loss 0.9854 | AdvPrec1 61.557 | AdvPrec5 97.394 | Reg term: 0.0\n",
      "Train Epoch:89 | Loss 0.9845 | AdvPrec1 61.650 | AdvPrec5 97.400 | Reg term: 0.0\n",
      "Train Epoch:90 | Loss 0.9730 | AdvPrec1 61.954 | AdvPrec5 97.562 | Reg term: 0.0\n",
      "Val Epoch:90 | Loss 0.7425 | NatPrec1 74.460 | NatPrec5 97.996 | Reg term: 0.0 |\n",
      "Val Epoch:90 | Loss 1.3885 | AdvPrec1 51.067 | AdvPrec5 94.057 | Reg term: 0.0 |\n",
      "Train Epoch:91 | Loss 0.9691 | AdvPrec1 62.117 | AdvPrec5 97.488 | Reg term: 0.0\n",
      "Train Epoch:92 | Loss 0.9653 | AdvPrec1 62.373 | AdvPrec5 97.593 | Reg term: 0.0\n",
      "Train Epoch:93 | Loss 0.9546 | AdvPrec1 62.586 | AdvPrec5 97.682 | Reg term: 0.0\n",
      "Train Epoch:94 | Loss 0.9490 | AdvPrec1 62.833 | AdvPrec5 97.718 | Reg term: 0.0\n",
      "Train Epoch:95 | Loss 0.9425 | AdvPrec1 63.012 | AdvPrec5 97.757 | Reg term: 0.0\n",
      "Val Epoch:95 | Loss 0.7349 | NatPrec1 74.726 | NatPrec5 97.997 | Reg term: 0.0 |\n",
      "Val Epoch:95 | Loss 1.4643 | AdvPrec1 50.258 | AdvPrec5 93.973 | Reg term: 0.0 |\n",
      "Train Epoch:96 | Loss 0.9400 | AdvPrec1 63.268 | AdvPrec5 97.776 | Reg term: 0.0\n",
      "Train Epoch:97 | Loss 0.9336 | AdvPrec1 63.174 | AdvPrec5 97.883 | Reg term: 0.0\n",
      "Train Epoch:98 | Loss 0.9323 | AdvPrec1 63.448 | AdvPrec5 97.850 | Reg term: 0.0\n",
      "Train Epoch:99 | Loss 0.9269 | AdvPrec1 63.586 | AdvPrec5 97.957 | Reg term: 0.0\n",
      "Train Epoch:100 | Loss 0.7517 | AdvPrec1 70.288 | AdvPrec5 98.810 | Reg term: 0.\n",
      "Val Epoch:100 | Loss 0.6589 | NatPrec1 77.390 | NatPrec5 98.219 | Reg term: 0.0 \n",
      "Val Epoch:100 | Loss 1.4983 | AdvPrec1 52.748 | AdvPrec5 94.513 | Reg term: 0.0 \n",
      "Train Epoch:101 | Loss 0.6754 | AdvPrec1 73.022 | AdvPrec5 99.144 | Reg term: 0.\n",
      "Train Epoch:102 | Loss 0.6420 | AdvPrec1 74.130 | AdvPrec5 99.237 | Reg term: 0.\n",
      "Train Epoch:103 | Loss 0.6215 | AdvPrec1 75.018 | AdvPrec5 99.304 | Reg term: 0.\n",
      "Train Epoch:104 | Loss 0.6019 | AdvPrec1 75.502 | AdvPrec5 99.423 | Reg term: 0.\n",
      "Train Epoch:105 | Loss 0.5847 | AdvPrec1 76.240 | AdvPrec5 99.449 | Reg term: 0.\n",
      "Val Epoch:105 | Loss 0.7105 | NatPrec1 77.338 | NatPrec5 98.091 | Reg term: 0.0 \n",
      "Val Epoch:105 | Loss 1.8625 | AdvPrec1 50.777 | AdvPrec5 94.134 | Reg term: 0.0 \n",
      "Train Epoch:106 | Loss 0.5685 | AdvPrec1 76.759 | AdvPrec5 99.497 | Reg term: 0.\n",
      "Train Epoch:107 | Loss 0.5543 | AdvPrec1 77.371 | AdvPrec5 99.502 | Reg term: 0.\n",
      "Train Epoch:108 | Loss 0.5430 | AdvPrec1 77.934 | AdvPrec5 99.559 | Reg term: 0.\n",
      "Train Epoch:109 | Loss 0.5195 | AdvPrec1 78.723 | AdvPrec5 99.589 | Reg term: 0."
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename to   CINIC-50Store-ADV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
