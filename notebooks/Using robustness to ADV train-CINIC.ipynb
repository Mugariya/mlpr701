{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_DIR = '/tmp/'\n",
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ADV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashraf.haddad/.conda/envs/rob/lib/python3.9/site-packages/robustness/train.py:24: UserWarning: Could not import amp.\n",
      "  warnings.warn('Could not import amp.')\n"
     ]
    }
   ],
   "source": [
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.datasets import CINIC#CIFAR#FashionMnist\n",
    "from robustness import data_augmentation as da\n",
    "import torch \n",
    "import torchvision.datasets\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('/home/u21010246/mlpr/venv/lib/python3.8/site-packages/robustness')\n",
    "\n",
    "# We use cox (http://github.com/MadryLab/cox) to log, store and analyze\n",
    "# results. Read more at https//cox.readthedocs.io.\n",
    "from cox.utils import Parameters\n",
    "import cox.store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cinic..\n"
     ]
    }
   ],
   "source": [
    "# Hard-coded dataset, architecture, batch size, workers\n",
    "ds = CINIC('/tmp/cinic') # CIFAR('/tmp')\n",
    "m, _ = model_utils.make_and_restore_model(arch='resnet18', dataset=ds)  #same model as std training \n",
    "train_loader, val_loader = ds.make_loaders(batch_size=BATCH_SIZE, workers=NUM_WORKERS)\n",
    "\n",
    "#wget http://data.csail.mit.edu/places/places365/places365standard_easyformat.tar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a cox store for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /tmp/35daedae-1b39-4941-ad08-8bd6459c1bd8\n"
     ]
    }
   ],
   "source": [
    "# Create a cox store for logging  , later useful for tensboard to visualize the accuracy curve over epochs\n",
    "out_store = cox.store.Store(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded base parameters\n",
    "train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'adv_train': 1,\n",
    "    'constraint': '2',\n",
    "    'eps': 0.5,\n",
    "    'attack_lr': 0.1,\n",
    "    'attack_steps': 7,\n",
    "    'epochs': 120\n",
    "}\n",
    "train_args = Parameters(train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.TRAINING_ARGS, CINIC)\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.PGD_ARGS, CINIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 | Loss 2.1778 | AdvPrec1 19.433 | AdvPrec5 70.091 | Reg term: 0.0 \n",
      "Val Epoch:0 | Loss 1.7935 | NatPrec1 33.681 | NatPrec5 86.427 | Reg term: 0.0 ||\n",
      "Val Epoch:0 | Loss 1.9595 | AdvPrec1 26.708 | AdvPrec5 80.396 | Reg term: 0.0 ||\n",
      "Train Epoch:1 | Loss 1.9437 | AdvPrec1 26.536 | AdvPrec5 80.501 | Reg term: 0.0 \n",
      "Train Epoch:2 | Loss 1.8464 | AdvPrec1 30.532 | AdvPrec5 84.014 | Reg term: 0.0 \n",
      "Train Epoch:3 | Loss 1.7631 | AdvPrec1 33.533 | AdvPrec5 86.297 | Reg term: 0.0 \n",
      "Train Epoch:4 | Loss 1.7034 | AdvPrec1 35.888 | AdvPrec5 87.636 | Reg term: 0.0 \n",
      "Train Epoch:5 | Loss 1.6615 | AdvPrec1 37.356 | AdvPrec5 88.523 | Reg term: 0.0 \n",
      "Val Epoch:5 | Loss 1.3063 | NatPrec1 52.361 | NatPrec5 94.189 | Reg term: 0.0 ||\n",
      "Val Epoch:5 | Loss 1.6219 | AdvPrec1 38.840 | AdvPrec5 89.630 | Reg term: 0.0 ||\n",
      "Train Epoch:6 | Loss 1.6329 | AdvPrec1 38.429 | AdvPrec5 89.090 | Reg term: 0.0 \n",
      "Train Epoch:7 | Loss 1.6067 | AdvPrec1 39.404 | AdvPrec5 89.570 | Reg term: 0.0 \n",
      "Train Epoch:8 | Loss 1.5875 | AdvPrec1 40.273 | AdvPrec5 89.804 | Reg term: 0.0 \n",
      "Train Epoch:9 | Loss 1.5708 | AdvPrec1 41.043 | AdvPrec5 90.104 | Reg term: 0.0 \n",
      "Train Epoch:10 | Loss 1.5589 | AdvPrec1 41.300 | AdvPrec5 90.317 | Reg term: 0.0\n",
      "Val Epoch:10 | Loss 1.2490 | NatPrec1 55.322 | NatPrec5 94.513 | Reg term: 0.0 |\n",
      "Val Epoch:10 | Loss 1.6317 | AdvPrec1 39.067 | AdvPrec5 89.679 | Reg term: 0.0 |\n",
      "Train Epoch:11 | Loss 1.5481 | AdvPrec1 41.876 | AdvPrec5 90.476 | Reg term: 0.0\n",
      "Train Epoch:12 | Loss 1.5362 | AdvPrec1 42.173 | AdvPrec5 90.671 | Reg term: 0.0\n",
      "Train Epoch:13 | Loss 1.5250 | AdvPrec1 42.738 | AdvPrec5 90.828 | Reg term: 0.0\n",
      "Train Epoch:14 | Loss 1.5165 | AdvPrec1 42.982 | AdvPrec5 91.000 | Reg term: 0.0\n",
      "Train Epoch:15 | Loss 1.5089 | AdvPrec1 43.312 | AdvPrec5 91.150 | Reg term: 0.0\n",
      "Val Epoch:15 | Loss 1.1463 | NatPrec1 60.210 | NatPrec5 95.774 | Reg term: 0.0 |\n",
      "Val Epoch:15 | Loss 1.5057 | AdvPrec1 43.058 | AdvPrec5 91.243 | Reg term: 0.0 |\n",
      "Train Epoch:16 | Loss 1.5061 | AdvPrec1 43.430 | AdvPrec5 91.200 | Reg term: 0.0\n",
      "Train Epoch:17 | Loss 1.5019 | AdvPrec1 43.816 | AdvPrec5 91.150 | Reg term: 0.0\n",
      "Train Epoch:18 | Loss 1.4959 | AdvPrec1 43.854 | AdvPrec5 91.337 | Reg term: 0.0\n",
      "Train Epoch:19 | Loss 1.4934 | AdvPrec1 44.109 | AdvPrec5 91.383 | Reg term: 0.0\n",
      "Train Epoch:20 | Loss 1.4879 | AdvPrec1 44.141 | AdvPrec5 91.520 | Reg term: 0.0\n",
      "Val Epoch:20 | Loss 1.1250 | NatPrec1 59.903 | NatPrec5 95.623 | Reg term: 0.0 |\n",
      "Val Epoch:20 | Loss 1.5363 | AdvPrec1 42.738 | AdvPrec5 90.996 | Reg term: 0.0 |\n",
      "Train Epoch:21 | Loss 1.4821 | AdvPrec1 44.344 | AdvPrec5 91.638 | Reg term: 0.0\n",
      "Train Epoch:22 | Loss 1.4800 | AdvPrec1 44.377 | AdvPrec5 91.688 | Reg term: 0.0\n",
      "Train Epoch:23 | Loss 1.4778 | AdvPrec1 44.511 | AdvPrec5 91.584 | Reg term: 0.0\n",
      "Train Epoch:24 | Loss 1.4745 | AdvPrec1 44.659 | AdvPrec5 91.753 | Reg term: 0.0\n",
      "Train Epoch:25 | Loss 1.4720 | AdvPrec1 44.830 | AdvPrec5 91.657 | Reg term: 0.0\n",
      "Val Epoch:25 | Loss 1.0864 | NatPrec1 62.840 | NatPrec5 96.170 | Reg term: 0.0 |\n",
      "Val Epoch:25 | Loss 1.4713 | AdvPrec1 44.420 | AdvPrec5 91.662 | Reg term: 0.0 |\n",
      "Train Epoch:26 | Loss 1.4714 | AdvPrec1 44.742 | AdvPrec5 91.652 | Reg term: 0.0\n",
      "Train Epoch:27 | Loss 1.4679 | AdvPrec1 45.103 | AdvPrec5 91.654 | Reg term: 0.0\n",
      "Train Epoch:28 | Loss 1.4673 | AdvPrec1 44.861 | AdvPrec5 91.813 | Reg term: 0.0\n",
      "Train Epoch:29 | Loss 1.4619 | AdvPrec1 45.322 | AdvPrec5 91.887 | Reg term: 0.0\n",
      "Train Epoch:30 | Loss 1.4629 | AdvPrec1 45.122 | AdvPrec5 91.814 | Reg term: 0.0\n",
      "Val Epoch:30 | Loss 1.0951 | NatPrec1 61.061 | NatPrec5 96.114 | Reg term: 0.0 |\n",
      "Val Epoch:30 | Loss 1.5478 | AdvPrec1 41.384 | AdvPrec5 91.364 | Reg term: 0.0 |\n",
      "Train Epoch:31 | Loss 1.4613 | AdvPrec1 45.240 | AdvPrec5 91.758 | Reg term: 0.0\n",
      "Train Epoch:32 | Loss 1.4613 | AdvPrec1 45.163 | AdvPrec5 91.933 | Reg term: 0.0\n",
      "Train Epoch:33 | Loss 1.4580 | AdvPrec1 45.306 | AdvPrec5 91.954 | Reg term: 0.0\n",
      "Train Epoch:34 | Loss 1.4579 | AdvPrec1 45.371 | AdvPrec5 91.868 | Reg term: 0.0\n",
      "Train Epoch:35 | Loss 1.4546 | AdvPrec1 45.463 | AdvPrec5 91.887 | Reg term: 0.0\n",
      "Val Epoch:35 | Loss 1.0991 | NatPrec1 60.641 | NatPrec5 96.007 | Reg term: 0.0 |\n",
      "Val Epoch:35 | Loss 1.5520 | AdvPrec1 42.471 | AdvPrec5 91.339 | Reg term: 0.0 |\n",
      "Train Epoch:36 | Loss 1.4549 | AdvPrec1 45.350 | AdvPrec5 91.937 | Reg term: 0.0\n",
      "Train Epoch:37 | Loss 1.4521 | AdvPrec1 45.453 | AdvPrec5 91.976 | Reg term: 0.0\n",
      "Train Epoch:38 | Loss 1.4532 | AdvPrec1 45.560 | AdvPrec5 91.991 | Reg term: 0.0\n",
      "Train Epoch:39 | Loss 1.4517 | AdvPrec1 45.594 | AdvPrec5 92.031 | Reg term: 0.0\n",
      "Train Epoch:40 | Loss 1.4502 | AdvPrec1 45.467 | AdvPrec5 91.994 | Reg term: 0.0\n",
      "Val Epoch:40 | Loss 1.0660 | NatPrec1 61.689 | NatPrec5 96.274 | Reg term: 0.0 |\n",
      "Val Epoch:40 | Loss 1.5033 | AdvPrec1 43.248 | AdvPrec5 91.921 | Reg term: 0.0 |\n",
      "Train Epoch:41 | Loss 1.4514 | AdvPrec1 45.458 | AdvPrec5 92.053 | Reg term: 0.0\n",
      "Train Epoch:42 | Loss 1.4489 | AdvPrec1 45.601 | AdvPrec5 92.042 | Reg term: 0.0\n",
      "Train Epoch:43 | Loss 1.4462 | AdvPrec1 45.756 | AdvPrec5 92.023 | Reg term: 0.0\n",
      "Train Epoch:44 | Loss 1.4454 | AdvPrec1 45.891 | AdvPrec5 92.038 | Reg term: 0.0\n",
      "Train Epoch:45 | Loss 1.4473 | AdvPrec1 45.887 | AdvPrec5 91.964 | Reg term: 0.0\n",
      "Val Epoch:45 | Loss 1.0524 | NatPrec1 61.893 | NatPrec5 96.432 | Reg term: 0.0 |\n",
      "Val Epoch:45 | Loss 1.4599 | AdvPrec1 44.811 | AdvPrec5 92.391 | Reg term: 0.0 |\n",
      "Train Epoch:46 | Loss 1.4453 | AdvPrec1 45.724 | AdvPrec5 92.046 | Reg term: 0.0\n",
      "Train Epoch:47 | Loss 1.4422 | AdvPrec1 46.129 | AdvPrec5 91.993 | Reg term: 0.0\n",
      "Train Epoch:48 | Loss 1.4438 | AdvPrec1 45.820 | AdvPrec5 92.162 | Reg term: 0.0\n",
      "Train Epoch:49 | Loss 1.4440 | AdvPrec1 45.829 | AdvPrec5 92.046 | Reg term: 0.0\n",
      "Train Epoch:50 | Loss 1.2850 | AdvPrec1 51.603 | AdvPrec5 94.083 | Reg term: 0.0\n",
      "Val Epoch:50 | Loss 0.8388 | NatPrec1 71.619 | NatPrec5 97.669 | Reg term: 0.0 |\n",
      "Val Epoch:50 | Loss 1.2587 | AdvPrec1 52.720 | AdvPrec5 94.469 | Reg term: 0.0 |\n",
      "Train Epoch:51 | Loss 1.2321 | AdvPrec1 53.504 | AdvPrec5 94.740 | Reg term: 0.0\n",
      "Train Epoch:52 | Loss 1.2106 | AdvPrec1 54.156 | AdvPrec5 95.021 | Reg term: 0.0\n",
      "Train Epoch:53 | Loss 1.1937 | AdvPrec1 54.767 | AdvPrec5 95.150 | Reg term: 0.0\n",
      "Train Epoch:54 | Loss 1.1784 | AdvPrec1 55.310 | AdvPrec5 95.341 | Reg term: 0.0\n",
      "Train Epoch:55 | Loss 1.1683 | AdvPrec1 55.627 | AdvPrec5 95.420 | Reg term: 0.0\n",
      "Val Epoch:55 | Loss 0.7765 | NatPrec1 73.396 | NatPrec5 98.023 | Reg term: 0.0 |\n",
      "Val Epoch:55 | Loss 1.2390 | AdvPrec1 53.632 | AdvPrec5 94.757 | Reg term: 0.0 |\n",
      "Train Epoch:56 | Loss 1.1581 | AdvPrec1 56.040 | AdvPrec5 95.591 | Reg term: 0.0\n",
      "Train Epoch:57 | Loss 1.1494 | AdvPrec1 56.381 | AdvPrec5 95.759 | Reg term: 0.0\n",
      "Train Epoch:58 | Loss 1.1409 | AdvPrec1 56.527 | AdvPrec5 95.800 | Reg term: 0.0\n",
      "Train Epoch:59 | Loss 1.1337 | AdvPrec1 56.739 | AdvPrec5 95.906 | Reg term: 0.0\n",
      "Train Epoch:60 | Loss 1.1279 | AdvPrec1 57.133 | AdvPrec5 95.960 | Reg term: 0.0\n",
      "Val Epoch:60 | Loss 0.7616 | NatPrec1 73.576 | NatPrec5 97.929 | Reg term: 0.0 |\n",
      "Val Epoch:60 | Loss 1.2636 | AdvPrec1 53.483 | AdvPrec5 94.712 | Reg term: 0.0 |\n",
      "Train Epoch:61 | Loss 1.1243 | AdvPrec1 57.103 | AdvPrec5 95.988 | Reg term: 0.0\n",
      "Train Epoch:62 | Loss 1.1175 | AdvPrec1 57.429 | AdvPrec5 96.141 | Reg term: 0.0\n",
      "Train Epoch:63 | Loss 1.1113 | AdvPrec1 57.480 | AdvPrec5 96.128 | Reg term: 0.0\n",
      "Train Epoch:64 | Loss 1.1046 | AdvPrec1 57.629 | AdvPrec5 96.197 | Reg term: 0.0\n",
      "Train Epoch:65 | Loss 1.1001 | AdvPrec1 57.851 | AdvPrec5 96.421 | Reg term: 0.0\n",
      "Val Epoch:65 | Loss 0.7676 | NatPrec1 73.464 | NatPrec5 97.879 | Reg term: 0.0 |\n",
      "Val Epoch:65 | Loss 1.2881 | AdvPrec1 52.529 | AdvPrec5 94.477 | Reg term: 0.0 |\n",
      "Train Epoch:66 | Loss 1.0935 | AdvPrec1 58.106 | AdvPrec5 96.357 | Reg term: 0.0\n",
      "Train Epoch:67 | Loss 1.0856 | AdvPrec1 58.303 | AdvPrec5 96.459 | Reg term: 0.0\n",
      "Train Epoch:68 | Loss 1.0808 | AdvPrec1 58.387 | AdvPrec5 96.519 | Reg term: 0.0\n",
      "Train Epoch:69 | Loss 1.0747 | AdvPrec1 58.496 | AdvPrec5 96.626 | Reg term: 0.0\n",
      "Train Epoch:70 | Loss 1.0725 | AdvPrec1 58.762 | AdvPrec5 96.639 | Reg term: 0.0\n",
      "Val Epoch:70 | Loss 0.7597 | NatPrec1 73.560 | NatPrec5 97.917 | Reg term: 0.0 |\n",
      "Val Epoch:70 | Loss 1.2909 | AdvPrec1 53.124 | AdvPrec5 94.398 | Reg term: 0.0 |\n",
      "Train Epoch:71 | Loss 1.0648 | AdvPrec1 58.776 | AdvPrec5 96.777 | Reg term: 0.0\n",
      "Train Epoch:72 | Loss 1.0610 | AdvPrec1 58.866 | AdvPrec5 96.754 | Reg term: 0.0\n",
      "Train Epoch:73 | Loss 1.0534 | AdvPrec1 59.248 | AdvPrec5 96.837 | Reg term: 0.0\n",
      "Train Epoch:74 | Loss 1.0431 | AdvPrec1 59.433 | AdvPrec5 97.032 | Reg term: 0.0\n",
      "Train Epoch:75 | Loss 1.0440 | AdvPrec1 59.487 | AdvPrec5 97.017 | Reg term: 0.0\n",
      "Val Epoch:75 | Loss 0.7611 | NatPrec1 73.712 | NatPrec5 97.834 | Reg term: 0.0 |\n",
      "Val Epoch:75 | Loss 1.3848 | AdvPrec1 51.217 | AdvPrec5 93.638 | Reg term: 0.0 |\n",
      "Train Epoch:76 | Loss 1.0329 | AdvPrec1 59.874 | AdvPrec5 97.123 | Reg term: 0.0\n",
      "Train Epoch:77 | Loss 1.0368 | AdvPrec1 59.707 | AdvPrec5 97.088 | Reg term: 0.0\n",
      "Train Epoch:78 | Loss 1.0245 | AdvPrec1 60.030 | AdvPrec5 97.220 | Reg term: 0.0\n",
      "Train Epoch:79 | Loss 1.0158 | AdvPrec1 60.387 | AdvPrec5 97.311 | Reg term: 0.0\n",
      "Train Epoch:80 | Loss 1.0131 | AdvPrec1 60.296 | AdvPrec5 97.316 | Reg term: 0.0\n",
      "Val Epoch:80 | Loss 0.7468 | NatPrec1 73.747 | NatPrec5 97.971 | Reg term: 0.0 |\n",
      "Val Epoch:80 | Loss 1.3819 | AdvPrec1 51.518 | AdvPrec5 93.883 | Reg term: 0.0 |\n",
      "Train Epoch:81 | Loss 1.0071 | AdvPrec1 60.581 | AdvPrec5 97.386 | Reg term: 0.0\n",
      "Train Epoch:82 | Loss 1.0003 | AdvPrec1 60.953 | AdvPrec5 97.414 | Reg term: 0.0\n",
      "Train Epoch:83 | Loss 0.9933 | AdvPrec1 61.060 | AdvPrec5 97.480 | Reg term: 0.0\n",
      "Train Epoch:84 | Loss 0.9904 | AdvPrec1 61.218 | AdvPrec5 97.519 | Reg term: 0.0\n",
      "Train Epoch:85 | Loss 0.9817 | AdvPrec1 61.296 | AdvPrec5 97.644 | Reg term: 0.0\n",
      "Val Epoch:85 | Loss 0.7529 | NatPrec1 73.898 | NatPrec5 97.776 | Reg term: 0.0 |\n",
      "Val Epoch:85 | Loss 1.4180 | AdvPrec1 50.651 | AdvPrec5 93.492 | Reg term: 0.0 |\n",
      "Train Epoch:86 | Loss 0.9768 | AdvPrec1 61.712 | AdvPrec5 97.647 | Reg term: 0.0\n",
      "Train Epoch:87 | Loss 0.9738 | AdvPrec1 61.472 | AdvPrec5 97.720 | Reg term: 0.0\n",
      "Train Epoch:88 | Loss 0.9646 | AdvPrec1 61.929 | AdvPrec5 97.780 | Reg term: 0.0\n",
      "Train Epoch:89 | Loss 0.9615 | AdvPrec1 62.150 | AdvPrec5 97.829 | Reg term: 0.0\n",
      "Train Epoch:90 | Loss 0.9562 | AdvPrec1 62.230 | AdvPrec5 97.878 | Reg term: 0.0\n",
      "Val Epoch:90 | Loss 0.7601 | NatPrec1 73.338 | NatPrec5 97.826 | Reg term: 0.0 |\n",
      "Val Epoch:90 | Loss 1.4066 | AdvPrec1 51.329 | AdvPrec5 93.659 | Reg term: 0.0 |\n",
      "Train Epoch:91 | Loss 0.9558 | AdvPrec1 62.279 | AdvPrec5 97.974 | Reg term: 0.0\n",
      "Train Epoch:92 | Loss 0.9468 | AdvPrec1 62.596 | AdvPrec5 98.024 | Reg term: 0.0\n",
      "Train Epoch:93 | Loss 0.9451 | AdvPrec1 62.680 | AdvPrec5 98.019 | Reg term: 0.0\n",
      "Train Epoch:94 | Loss 0.9341 | AdvPrec1 63.004 | AdvPrec5 98.047 | Reg term: 0.0\n",
      "Train Epoch:95 | Loss 0.9362 | AdvPrec1 62.879 | AdvPrec5 98.074 | Reg term: 0.0\n",
      "Val Epoch:95 | Loss 0.7425 | NatPrec1 74.348 | NatPrec5 97.851 | Reg term: 0.0 |\n",
      "Val Epoch:95 | Loss 1.4789 | AdvPrec1 50.949 | AdvPrec5 93.280 | Reg term: 0.0 |\n",
      "Train Epoch:96 | Loss 0.9282 | AdvPrec1 62.991 | AdvPrec5 98.059 | Reg term: 0.0\n",
      "Train Epoch:97 | Loss 0.9251 | AdvPrec1 63.158 | AdvPrec5 98.134 | Reg term: 0.0\n",
      "Train Epoch:98 | Loss 0.9160 | AdvPrec1 63.493 | AdvPrec5 98.294 | Reg term: 0.0\n",
      "Train Epoch:99 | Loss 0.9095 | AdvPrec1 63.692 | AdvPrec5 98.239 | Reg term: 0.0\n",
      "Train Epoch:100 | Loss 0.7581 | AdvPrec1 69.788 | AdvPrec5 99.016 | Reg term: 0.\n",
      "Val Epoch:100 | Loss 0.6874 | NatPrec1 76.410 | NatPrec5 98.060 | Reg term: 0.0 \n",
      "Val Epoch:100 | Loss 1.4810 | AdvPrec1 52.862 | AdvPrec5 93.787 | Reg term: 0.0 \n",
      "Train Epoch:101 | Loss 0.6961 | AdvPrec1 72.054 | AdvPrec5 99.287 | Reg term: 0.\n",
      "Train Epoch:102 | Loss 0.6655 | AdvPrec1 73.178 | AdvPrec5 99.356 | Reg term: 0.\n",
      "Train Epoch:103 | Loss 0.6442 | AdvPrec1 73.724 | AdvPrec5 99.429 | Reg term: 0.\n",
      "Train Epoch:104 | Loss 0.6291 | AdvPrec1 74.451 | AdvPrec5 99.481 | Reg term: 0.\n",
      "Train Epoch:105 | Loss 0.6144 | AdvPrec1 74.883 | AdvPrec5 99.530 | Reg term: 0.\n",
      "Val Epoch:105 | Loss 0.7151 | NatPrec1 76.412 | NatPrec5 97.946 | Reg term: 0.0 \n",
      "Val Epoch:105 | Loss 1.6929 | AdvPrec1 51.591 | AdvPrec5 93.453 | Reg term: 0.0 \n",
      "Train Epoch:106 | Loss 0.6018 | AdvPrec1 75.407 | AdvPrec5 99.549 | Reg term: 0.\n",
      "Train Epoch:107 | Loss 0.5897 | AdvPrec1 75.790 | AdvPrec5 99.573 | Reg term: 0.\n",
      "Train Epoch:108 | Loss 0.5800 | AdvPrec1 76.128 | AdvPrec5 99.587 | Reg term: 0.\n",
      "Train Epoch:109 | Loss 0.5699 | AdvPrec1 76.663 | AdvPrec5 99.604 | Reg term: 0.\n",
      "Train Epoch:110 | Loss 0.5584 | AdvPrec1 76.937 | AdvPrec5 99.667 | Reg term: 0.\n",
      "Val Epoch:110 | Loss 0.7410 | NatPrec1 76.233 | NatPrec5 97.877 | Reg term: 0.0 \n",
      "Val Epoch:110 | Loss 1.8514 | AdvPrec1 50.368 | AdvPrec5 93.223 | Reg term: 0.0 \n",
      "Train Epoch:111 | Loss 0.5481 | AdvPrec1 77.416 | AdvPrec5 99.680 | Reg term: 0.\n",
      "Train Epoch:112 | Loss 0.5414 | AdvPrec1 77.627 | AdvPrec5 99.724 | Reg term: 0.\n",
      "Train Epoch:113 | Loss 0.5318 | AdvPrec1 78.076 | AdvPrec5 99.692 | Reg term: 0.\n",
      "Train Epoch:114 | Loss 0.5231 | AdvPrec1 78.424 | AdvPrec5 99.692 | Reg term: 0.\n",
      "Train Epoch:115 | Loss 0.5176 | AdvPrec1 78.662 | AdvPrec5 99.703 | Reg term: 0.\n",
      "Val Epoch:115 | Loss 0.7679 | NatPrec1 76.208 | NatPrec5 97.793 | Reg term: 0.0 \n",
      "Val Epoch:115 | Loss 1.9669 | AdvPrec1 50.040 | AdvPrec5 92.992 | Reg term: 0.0 \n",
      "Train Epoch:116 | Loss 0.5105 | AdvPrec1 78.924 | AdvPrec5 99.699 | Reg term: 0.\n",
      "Train Epoch:117 | Loss 0.5001 | AdvPrec1 79.370 | AdvPrec5 99.747 | Reg term: 0.\n",
      "Train Epoch:118 | Loss 0.4963 | AdvPrec1 79.556 | AdvPrec5 99.748 | Reg term: 0.\n",
      "Train Epoch:119 | Loss 0.4886 | AdvPrec1 79.813 | AdvPrec5 99.769 | Reg term: 0.\n",
      "Val Epoch:119 | Loss 0.7962 | NatPrec1 75.838 | NatPrec5 97.713 | Reg term: 0.0 \n",
      "Val Epoch:119 | Loss 2.0610 | AdvPrec1 49.593 | AdvPrec5 92.872 | Reg term: 0.0 \n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
