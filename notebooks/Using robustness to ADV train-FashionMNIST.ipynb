{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = '/tmp/'\n",
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# The following notebook is adapted from\n",
    "# https://github.com/MadryLab/robustness/blob/master/notebooks/Using%20robustness%20as%20a%20library.ipynb\n",
    "\n",
    "\n",
    "from torchvision.models.utils import load_state_dict_from_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashraf.haddad/.conda/envs/rob/lib/python3.9/site-packages/robustness/train.py:24: UserWarning: Could not import amp.\n",
      "  warnings.warn('Could not import amp.')\n"
     ]
    }
   ],
   "source": [
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.datasets import FashionMnist#CIFAR#FashionMnist\n",
    "from robustness import data_augmentation as da\n",
    "import torch \n",
    "import torchvision.datasets\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('/home/u21010246/mlpr/venv/lib/python3.8/site-packages/robustness')\n",
    "\n",
    "# We use cox (http://github.com/MadryLab/cox) to log, store and analyze\n",
    "# results. Read more at https//cox.readthedocs.io.\n",
    "from cox.utils import Parameters\n",
    "import cox.store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /tmp/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a0599f151d4c2a97e0e6ffbfc853e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/FashionMNIST/raw/train-images-idx3-ubyte.gz to /tmp/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /tmp/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20681e431708489c96406f4b0cd3bc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /tmp/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /tmp/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628053bd3145400cb73ef081515b50aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /tmp/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1a7c1e4c6049d1a1eec484cd229cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashraf.haddad/.conda/envs/rob/lib/python3.9/site-packages/torchvision/datasets/mnist.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378073850/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "#download the dataset\n",
    "fm_train = torchvision.datasets.FashionMNIST('/tmp', download=True, train=True,transform=da.TRAIN_TRANSFORMS_DEFAULT(32))\n",
    "fm_val = torchvision.datasets.FashionMNIST('/tmp', download=True, train=False,transform=da.TEST_TRANSFORMS_DEFAULT(32))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(fm_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = torch.utils.data.DataLoader(fm_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "ds = FashionMnist('/tmp/FashionMNIST/raw') # CIFAR('/tmp')\n",
    "m, _ = model_utils.make_and_restore_model(arch='resnet18', dataset=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a cox store for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /tmp/ea313207-b970-4a7e-877b-cd77e5e79a24\n"
     ]
    }
   ],
   "source": [
    "# Create a cox store for logging\n",
    "out_store = cox.store.Store(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded base parameters\n",
    "train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'adv_train': 1,\n",
    "    'constraint': '2',\n",
    "    'eps': 0.5,\n",
    "    'attack_lr': 0.1,\n",
    "    'attack_steps': 7,\n",
    "    'epochs': 120\n",
    "}\n",
    "train_args = Parameters(train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.TRAINING_ARGS, FashionMnist)\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.PGD_ARGS, FashionMnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 | Loss 1.3718 | AdvPrec1 56.040 | AdvPrec5 94.698 | Reg term: 0.0 \n",
      "Val Epoch:0 | Loss 0.7390 | NatPrec1 69.720 | NatPrec5 99.390 | Reg term: 0.0 ||\n",
      "Val Epoch:0 | Loss 0.9120 | AdvPrec1 63.160 | AdvPrec5 99.090 | Reg term: 0.0 ||\n",
      "Train Epoch:1 | Loss 0.7063 | AdvPrec1 71.453 | AdvPrec5 99.390 | Reg term: 0.0 \n",
      "Train Epoch:2 | Loss 0.6158 | AdvPrec1 75.137 | AdvPrec5 99.553 | Reg term: 0.0 \n",
      "Train Epoch:3 | Loss 0.5807 | AdvPrec1 76.500 | AdvPrec5 99.620 | Reg term: 0.0 \n",
      "Train Epoch:4 | Loss 0.5471 | AdvPrec1 77.778 | AdvPrec5 99.685 | Reg term: 0.0 \n",
      "Train Epoch:5 | Loss 0.5234 | AdvPrec1 78.810 | AdvPrec5 99.748 | Reg term: 0.0 \n",
      "Val Epoch:5 | Loss 0.4089 | NatPrec1 84.060 | NatPrec5 99.830 | Reg term: 0.0 ||\n",
      "Val Epoch:5 | Loss 0.6014 | AdvPrec1 76.110 | AdvPrec5 99.680 | Reg term: 0.0 ||\n",
      "Train Epoch:6 | Loss 0.5045 | AdvPrec1 79.522 | AdvPrec5 99.752 | Reg term: 0.0 \n",
      "Train Epoch:7 | Loss 0.4925 | AdvPrec1 79.947 | AdvPrec5 99.788 | Reg term: 0.0 \n",
      "Train Epoch:8 | Loss 0.4797 | AdvPrec1 80.325 | AdvPrec5 99.813 | Reg term: 0.0 \n",
      "Train Epoch:9 | Loss 0.4716 | AdvPrec1 80.748 | AdvPrec5 99.858 | Reg term: 0.0 \n",
      "Train Epoch:10 | Loss 0.4626 | AdvPrec1 80.908 | AdvPrec5 99.855 | Reg term: 0.0\n",
      "Val Epoch:10 | Loss 0.3959 | NatPrec1 84.890 | NatPrec5 99.820 | Reg term: 0.0 |\n",
      "Val Epoch:10 | Loss 0.5996 | AdvPrec1 76.050 | AdvPrec5 99.680 | Reg term: 0.0 |\n",
      "Train Epoch:11 | Loss 0.4538 | AdvPrec1 81.370 | AdvPrec5 99.870 | Reg term: 0.0\n",
      "Train Epoch:12 | Loss 0.4448 | AdvPrec1 81.707 | AdvPrec5 99.862 | Reg term: 0.0\n",
      "Train Epoch:13 | Loss 0.4417 | AdvPrec1 81.985 | AdvPrec5 99.873 | Reg term: 0.0\n",
      "Train Epoch:14 | Loss 0.4335 | AdvPrec1 82.203 | AdvPrec5 99.875 | Reg term: 0.0\n",
      "Train Epoch:15 | Loss 0.4294 | AdvPrec1 82.347 | AdvPrec5 99.897 | Reg term: 0.0\n",
      "Val Epoch:15 | Loss 0.4098 | NatPrec1 83.430 | NatPrec5 99.960 | Reg term: 0.0 |\n",
      "Val Epoch:15 | Loss 0.6727 | AdvPrec1 72.760 | AdvPrec5 99.760 | Reg term: 0.0 |\n",
      "Train Epoch:16 | Loss 0.4262 | AdvPrec1 82.378 | AdvPrec5 99.902 | Reg term: 0.0\n",
      "Train Epoch:17 | Loss 0.4206 | AdvPrec1 82.643 | AdvPrec5 99.897 | Reg term: 0.0\n",
      "Train Epoch:18 | Loss 0.4170 | AdvPrec1 82.937 | AdvPrec5 99.902 | Reg term: 0.0\n",
      "Train Epoch:19 | Loss 0.4162 | AdvPrec1 82.683 | AdvPrec5 99.908 | Reg term: 0.0\n",
      "Train Epoch:20 | Loss 0.4089 | AdvPrec1 83.042 | AdvPrec5 99.902 | Reg term: 0.0\n",
      "Val Epoch:20 | Loss 0.3611 | NatPrec1 85.360 | NatPrec5 99.910 | Reg term: 0.0 |\n",
      "Val Epoch:20 | Loss 0.6000 | AdvPrec1 75.750 | AdvPrec5 99.820 | Reg term: 0.0 |\n",
      "Train Epoch:21 | Loss 0.4063 | AdvPrec1 83.392 | AdvPrec5 99.927 | Reg term: 0.0\n",
      "Train Epoch:22 | Loss 0.4013 | AdvPrec1 83.423 | AdvPrec5 99.920 | Reg term: 0.0\n",
      "Train Epoch:23 | Loss 0.4009 | AdvPrec1 83.318 | AdvPrec5 99.922 | Reg term: 0.0\n",
      "Train Epoch:24 | Loss 0.3985 | AdvPrec1 83.305 | AdvPrec5 99.955 | Reg term: 0.0\n",
      "Train Epoch:25 | Loss 0.3944 | AdvPrec1 83.652 | AdvPrec5 99.925 | Reg term: 0.0\n",
      "Val Epoch:25 | Loss 0.3866 | NatPrec1 84.670 | NatPrec5 99.860 | Reg term: 0.0 |\n",
      "Val Epoch:25 | Loss 0.6468 | AdvPrec1 74.970 | AdvPrec5 99.700 | Reg term: 0.0 |\n",
      "Train Epoch:26 | Loss 0.3913 | AdvPrec1 83.787 | AdvPrec5 99.932 | Reg term: 0.0\n",
      "Train Epoch:27 | Loss 0.3896 | AdvPrec1 83.845 | AdvPrec5 99.953 | Reg term: 0.0\n",
      "Train Epoch:28 | Loss 0.3887 | AdvPrec1 83.863 | AdvPrec5 99.952 | Reg term: 0.0\n",
      "Train Epoch:29 | Loss 0.3838 | AdvPrec1 83.945 | AdvPrec5 99.938 | Reg term: 0.0\n",
      "Train Epoch:30 | Loss 0.3811 | AdvPrec1 84.058 | AdvPrec5 99.953 | Reg term: 0.0\n",
      "Val Epoch:30 | Loss 0.3614 | NatPrec1 86.090 | NatPrec5 99.910 | Reg term: 0.0 |\n",
      "Val Epoch:30 | Loss 0.6285 | AdvPrec1 76.030 | AdvPrec5 99.740 | Reg term: 0.0 |\n",
      "Train Epoch:31 | Loss 0.3821 | AdvPrec1 84.022 | AdvPrec5 99.950 | Reg term: 0.0\n",
      "Train Epoch:32 | Loss 0.3790 | AdvPrec1 84.118 | AdvPrec5 99.963 | Reg term: 0.0\n",
      "Train Epoch:33 | Loss 0.3778 | AdvPrec1 84.197 | AdvPrec5 99.953 | Reg term: 0.0\n",
      "Train Epoch:34 | Loss 0.3746 | AdvPrec1 84.327 | AdvPrec5 99.973 | Reg term: 0.0\n",
      "Train Epoch:35 | Loss 0.3728 | AdvPrec1 84.387 | AdvPrec5 99.960 | Reg term: 0.0\n",
      "Val Epoch:35 | Loss 0.3354 | NatPrec1 86.610 | NatPrec5 99.930 | Reg term: 0.0 |\n",
      "Val Epoch:35 | Loss 0.5892 | AdvPrec1 77.440 | AdvPrec5 99.760 | Reg term: 0.0 |\n",
      "Train Epoch:36 | Loss 0.3704 | AdvPrec1 84.508 | AdvPrec5 99.962 | Reg term: 0.0\n",
      "Train Epoch:37 | Loss 0.3700 | AdvPrec1 84.615 | AdvPrec5 99.962 | Reg term: 0.0\n",
      "Train Epoch:38 | Loss 0.3668 | AdvPrec1 84.658 | AdvPrec5 99.962 | Reg term: 0.0\n",
      "Train Epoch:39 | Loss 0.3665 | AdvPrec1 84.778 | AdvPrec5 99.968 | Reg term: 0.0\n",
      "Train Epoch:40 | Loss 0.3655 | AdvPrec1 84.677 | AdvPrec5 99.957 | Reg term: 0.0\n",
      "Val Epoch:40 | Loss 0.4144 | NatPrec1 83.550 | NatPrec5 99.890 | Reg term: 0.0 |\n",
      "Val Epoch:40 | Loss 0.7328 | AdvPrec1 74.060 | AdvPrec5 99.750 | Reg term: 0.0 |\n",
      "Train Epoch:41 | Loss 0.3637 | AdvPrec1 84.825 | AdvPrec5 99.975 | Reg term: 0.0\n",
      "Train Epoch:42 | Loss 0.3624 | AdvPrec1 84.820 | AdvPrec5 99.967 | Reg term: 0.0\n",
      "Train Epoch:43 | Loss 0.3589 | AdvPrec1 84.835 | AdvPrec5 99.972 | Reg term: 0.0\n",
      "Train Epoch:44 | Loss 0.3579 | AdvPrec1 85.032 | AdvPrec5 99.967 | Reg term: 0.0\n",
      "Train Epoch:45 | Loss 0.3576 | AdvPrec1 85.050 | AdvPrec5 99.970 | Reg term: 0.0\n",
      "Val Epoch:45 | Loss 0.3312 | NatPrec1 86.370 | NatPrec5 99.920 | Reg term: 0.0 |\n",
      "Val Epoch:45 | Loss 0.6098 | AdvPrec1 75.920 | AdvPrec5 99.750 | Reg term: 0.0 |\n",
      "Train Epoch:46 | Loss 0.3540 | AdvPrec1 85.327 | AdvPrec5 99.973 | Reg term: 0.0\n",
      "Train Epoch:47 | Loss 0.3542 | AdvPrec1 85.133 | AdvPrec5 99.982 | Reg term: 0.0\n",
      "Train Epoch:48 | Loss 0.3527 | AdvPrec1 85.203 | AdvPrec5 99.980 | Reg term: 0.0\n",
      "Train Epoch:49 | Loss 0.3528 | AdvPrec1 85.180 | AdvPrec5 99.978 | Reg term: 0.0\n",
      "Train Epoch:50 | Loss 0.3063 | AdvPrec1 87.028 | AdvPrec5 99.988 | Reg term: 0.0\n",
      "Val Epoch:50 | Loss 0.3256 | NatPrec1 87.080 | NatPrec5 99.900 | Reg term: 0.0 |\n",
      "Val Epoch:50 | Loss 0.6210 | AdvPrec1 76.310 | AdvPrec5 99.770 | Reg term: 0.0 |\n",
      "Train Epoch:51 | Loss 0.2906 | AdvPrec1 87.795 | AdvPrec5 99.997 | Reg term: 0.0\n",
      "Train Epoch:52 | Loss 0.2827 | AdvPrec1 87.925 | AdvPrec5 99.992 | Reg term: 0.0\n",
      "Train Epoch:53 | Loss 0.2782 | AdvPrec1 88.172 | AdvPrec5 99.992 | Reg term: 0.0\n",
      "Train Epoch:54 | Loss 0.2746 | AdvPrec1 88.188 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:55 | Loss 0.2724 | AdvPrec1 88.387 | AdvPrec5 99.995 | Reg term: 0.0\n",
      "Val Epoch:55 | Loss 0.3508 | NatPrec1 86.560 | NatPrec5 99.890 | Reg term: 0.0 |\n",
      "Val Epoch:55 | Loss 0.6956 | AdvPrec1 75.830 | AdvPrec5 99.690 | Reg term: 0.0 |\n",
      "Train Epoch:56 | Loss 0.2683 | AdvPrec1 88.467 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:57 | Loss 0.2660 | AdvPrec1 88.520 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:58 | Loss 0.2620 | AdvPrec1 88.818 | AdvPrec5 99.997 | Reg term: 0.0\n",
      "Train Epoch:59 | Loss 0.2606 | AdvPrec1 88.865 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:60 | Loss 0.2587 | AdvPrec1 88.922 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Val Epoch:60 | Loss 0.3555 | NatPrec1 86.650 | NatPrec5 99.890 | Reg term: 0.0 |\n",
      "Val Epoch:60 | Loss 0.7367 | AdvPrec1 75.630 | AdvPrec5 99.690 | Reg term: 0.0 |\n",
      "Train Epoch:61 | Loss 0.2560 | AdvPrec1 89.010 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:62 | Loss 0.2531 | AdvPrec1 89.093 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:63 | Loss 0.2518 | AdvPrec1 89.182 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:64 | Loss 0.2480 | AdvPrec1 89.400 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:65 | Loss 0.2461 | AdvPrec1 89.295 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Val Epoch:65 | Loss 0.3927 | NatPrec1 85.740 | NatPrec5 99.850 | Reg term: 0.0 |\n",
      "Val Epoch:65 | Loss 0.8463 | AdvPrec1 74.690 | AdvPrec5 99.630 | Reg term: 0.0 |\n",
      "Train Epoch:66 | Loss 0.2447 | AdvPrec1 89.542 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:67 | Loss 0.2432 | AdvPrec1 89.518 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:68 | Loss 0.2390 | AdvPrec1 89.737 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:69 | Loss 0.2373 | AdvPrec1 89.803 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:70 | Loss 0.2357 | AdvPrec1 89.813 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Val Epoch:70 | Loss 0.3968 | NatPrec1 85.830 | NatPrec5 99.840 | Reg term: 0.0 |\n",
      "Val Epoch:70 | Loss 0.8854 | AdvPrec1 74.230 | AdvPrec5 99.560 | Reg term: 0.0 |\n",
      "Train Epoch:71 | Loss 0.2330 | AdvPrec1 89.795 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:72 | Loss 0.2296 | AdvPrec1 90.083 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:73 | Loss 0.2276 | AdvPrec1 90.177 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:74 | Loss 0.2254 | AdvPrec1 90.293 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:75 | Loss 0.2257 | AdvPrec1 90.268 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Val Epoch:75 | Loss 0.3760 | NatPrec1 87.040 | NatPrec5 99.820 | Reg term: 0.0 |\n",
      "Val Epoch:75 | Loss 0.8732 | AdvPrec1 74.750 | AdvPrec5 99.570 | Reg term: 0.0 |\n",
      "Train Epoch:76 | Loss 0.2223 | AdvPrec1 90.310 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:77 | Loss 0.2192 | AdvPrec1 90.493 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:78 | Loss 0.2198 | AdvPrec1 90.570 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:79 | Loss 0.2178 | AdvPrec1 90.607 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:80 | Loss 0.2147 | AdvPrec1 90.780 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Val Epoch:80 | Loss 0.3686 | NatPrec1 87.290 | NatPrec5 99.790 | Reg term: 0.0 |\n",
      "Val Epoch:80 | Loss 0.9034 | AdvPrec1 75.090 | AdvPrec5 99.430 | Reg term: 0.0 |\n",
      "Train Epoch:81 | Loss 0.2117 | AdvPrec1 90.868 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:82 | Loss 0.2109 | AdvPrec1 90.818 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:83 | Loss 0.2103 | AdvPrec1 90.952 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:84 | Loss 0.2070 | AdvPrec1 90.983 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:85 | Loss 0.2071 | AdvPrec1 91.030 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Val Epoch:85 | Loss 0.3950 | NatPrec1 87.250 | NatPrec5 99.820 | Reg term: 0.0 |\n",
      "Val Epoch:85 | Loss 0.9681 | AdvPrec1 73.790 | AdvPrec5 99.370 | Reg term: 0.0 |\n",
      "Train Epoch:86 | Loss 0.2034 | AdvPrec1 91.158 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:87 | Loss 0.2004 | AdvPrec1 91.283 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:88 | Loss 0.1993 | AdvPrec1 91.277 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:89 | Loss 0.1973 | AdvPrec1 91.425 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:90 | Loss 0.1964 | AdvPrec1 91.480 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Val Epoch:90 | Loss 0.4162 | NatPrec1 87.030 | NatPrec5 99.820 | Reg term: 0.0 |\n",
      "Val Epoch:90 | Loss 1.0322 | AdvPrec1 74.660 | AdvPrec5 99.320 | Reg term: 0.0 |\n",
      "Train Epoch:91 | Loss 0.1940 | AdvPrec1 91.578 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:92 | Loss 0.1936 | AdvPrec1 91.597 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:93 | Loss 0.1913 | AdvPrec1 91.812 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:94 | Loss 0.1898 | AdvPrec1 91.692 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:95 | Loss 0.1872 | AdvPrec1 91.892 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Val Epoch:95 | Loss 0.4464 | NatPrec1 86.820 | NatPrec5 99.830 | Reg term: 0.0 |\n",
      "Val Epoch:95 | Loss 1.1294 | AdvPrec1 74.480 | AdvPrec5 99.380 | Reg term: 0.0 |\n",
      "Train Epoch:96 | Loss 0.1879 | AdvPrec1 91.883 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:97 | Loss 0.1867 | AdvPrec1 91.908 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:98 | Loss 0.1856 | AdvPrec1 92.017 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:99 | Loss 0.1846 | AdvPrec1 92.055 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:100 | Loss 0.1592 | AdvPrec1 93.212 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:100 | Loss 0.4328 | NatPrec1 87.140 | NatPrec5 99.800 | Reg term: 0.0 \n",
      "Val Epoch:100 | Loss 1.1553 | AdvPrec1 73.610 | AdvPrec5 99.200 | Reg term: 0.0 \n",
      "Train Epoch:101 | Loss 0.1499 | AdvPrec1 93.652 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:102 | Loss 0.1460 | AdvPrec1 93.783 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:103 | Loss 0.1422 | AdvPrec1 93.993 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:104 | Loss 0.1403 | AdvPrec1 94.057 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:105 | Loss 0.1381 | AdvPrec1 94.208 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:105 | Loss 0.4591 | NatPrec1 87.230 | NatPrec5 99.780 | Reg term: 0.0 \n",
      "Val Epoch:105 | Loss 1.2631 | AdvPrec1 73.220 | AdvPrec5 99.050 | Reg term: 0.0 \n",
      "Train Epoch:106 | Loss 0.1369 | AdvPrec1 94.217 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:107 | Loss 0.1354 | AdvPrec1 94.342 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:108 | Loss 0.1342 | AdvPrec1 94.332 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:109 | Loss 0.1328 | AdvPrec1 94.353 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:110 | Loss 0.1312 | AdvPrec1 94.397 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:110 | Loss 0.4789 | NatPrec1 87.100 | NatPrec5 99.730 | Reg term: 0.0 \n",
      "Val Epoch:110 | Loss 1.3505 | AdvPrec1 73.010 | AdvPrec5 98.870 | Reg term: 0.0 \n",
      "Train Epoch:111 | Loss 0.1302 | AdvPrec1 94.507 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:112 | Loss 0.1291 | AdvPrec1 94.573 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:113 | Loss 0.1286 | AdvPrec1 94.580 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:114 | Loss 0.1271 | AdvPrec1 94.625 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:115 | Loss 0.1266 | AdvPrec1 94.573 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:115 | Loss 0.5066 | NatPrec1 87.100 | NatPrec5 99.720 | Reg term: 0.0 \n",
      "Val Epoch:115 | Loss 1.4315 | AdvPrec1 72.690 | AdvPrec5 98.840 | Reg term: 0.0 \n",
      "Train Epoch:116 | Loss 0.1251 | AdvPrec1 94.742 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:117 | Loss 0.1243 | AdvPrec1 94.733 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:118 | Loss 0.1227 | AdvPrec1 94.847 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:119 | Loss 0.1227 | AdvPrec1 94.815 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:119 | Loss 0.4941 | NatPrec1 87.090 | NatPrec5 99.660 | Reg term: 0.0 \n",
      "Val Epoch:119 | Loss 1.4389 | AdvPrec1 72.490 | AdvPrec5 98.890 | Reg term: 0.0 \n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
