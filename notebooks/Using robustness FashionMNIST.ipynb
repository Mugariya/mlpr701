{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUT_DIR = '/tmp/'\n",
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharim.jamal/.conda/envs/test/lib/python3.7/site-packages/robustness/train.py:24: UserWarning: Could not import amp.\n",
      "  warnings.warn('Could not import amp.')\n"
     ]
    }
   ],
   "source": [
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.datasets import FashionMnist#CIFAR#FashionMnist\n",
    "from robustness import data_augmentation as da\n",
    "import torch \n",
    "import torchvision.datasets\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('/home/u21010246/mlpr/venv/lib/python3.8/site-packages/robustness')\n",
    "\n",
    "# We use cox (http://github.com/MadryLab/cox) to log, store and analyze\n",
    "# results. Read more at https//cox.readthedocs.io.\n",
    "from cox.utils import Parameters\n",
    "import cox.store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the dataset\n",
    "fm_train = torchvision.datasets.FashionMNIST('/tmp', download=True, train=True,transform=da.TRAIN_TRANSFORMS_DEFAULT(32))\n",
    "fm_val = torchvision.datasets.FashionMNIST('/tmp', download=True, train=False,transform=da.TEST_TRANSFORMS_DEFAULT(32))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(fm_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = torch.utils.data.DataLoader(fm_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "ds = FashionMnist('/tmp/FashionMNIST') # FashionMNIST('/tmp')\n",
    "m, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a cox store for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /tmp/f7b12005-f2c2-4326-bc05-db52f6cf8599\n"
     ]
    }
   ],
   "source": [
    "# Create a cox store for logging\n",
    "out_store = cox.store.Store(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded base parameters\n",
    "train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'adv_train': 1,\n",
    "    'constraint': '2',\n",
    "    'eps': 0.5,\n",
    "    'attack_lr': 0.1,\n",
    "    'attack_steps': 7,\n",
    "    'epochs': 150\n",
    "}\n",
    "train_args = Parameters(train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.TRAINING_ARGS, FashionMnist)\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.PGD_ARGS, FashionMnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 | Loss 2.4520 | AdvPrec1 29.340 | AdvPrec5 78.653 | Reg term: 0.0 \n",
      "Val Epoch:0 | Loss 1.0299 | NatPrec1 61.380 | NatPrec5 98.750 | Reg term: 0.0 ||\n",
      "Val Epoch:0 | Loss 1.2124 | AdvPrec1 53.370 | AdvPrec5 97.750 | Reg term: 0.0 ||\n",
      "Train Epoch:1 | Loss 0.9891 | AdvPrec1 61.157 | AdvPrec5 98.502 | Reg term: 0.0 \n",
      "Train Epoch:2 | Loss 0.8642 | AdvPrec1 66.668 | AdvPrec5 99.108 | Reg term: 0.0 \n",
      "Train Epoch:3 | Loss 0.7627 | AdvPrec1 69.155 | AdvPrec5 99.302 | Reg term: 0.0 \n",
      "Train Epoch:4 | Loss 0.7045 | AdvPrec1 71.013 | AdvPrec5 99.417 | Reg term: 0.0 \n",
      "Train Epoch:5 | Loss 0.6568 | AdvPrec1 72.978 | AdvPrec5 99.542 | Reg term: 0.0 \n",
      "Val Epoch:5 | Loss 0.5597 | NatPrec1 76.110 | NatPrec5 99.720 | Reg term: 0.0 ||\n",
      "Val Epoch:5 | Loss 0.7249 | AdvPrec1 70.400 | AdvPrec5 99.490 | Reg term: 0.0 ||\n",
      "Train Epoch:6 | Loss 0.6262 | AdvPrec1 74.302 | AdvPrec5 99.573 | Reg term: 0.0 \n",
      "Train Epoch:7 | Loss 0.5990 | AdvPrec1 75.812 | AdvPrec5 99.587 | Reg term: 0.0 \n",
      "Train Epoch:8 | Loss 0.5752 | AdvPrec1 76.560 | AdvPrec5 99.685 | Reg term: 0.0 \n",
      "Train Epoch:9 | Loss 0.5586 | AdvPrec1 77.335 | AdvPrec5 99.690 | Reg term: 0.0 \n",
      "Train Epoch:10 | Loss 0.5463 | AdvPrec1 77.740 | AdvPrec5 99.710 | Reg term: 0.0\n",
      "Val Epoch:10 | Loss 0.4556 | NatPrec1 82.530 | NatPrec5 99.840 | Reg term: 0.0 |\n",
      "Val Epoch:10 | Loss 0.6553 | AdvPrec1 73.310 | AdvPrec5 99.700 | Reg term: 0.0 |\n",
      "Train Epoch:11 | Loss 0.5327 | AdvPrec1 78.322 | AdvPrec5 99.720 | Reg term: 0.0\n",
      "Train Epoch:12 | Loss 0.5201 | AdvPrec1 78.872 | AdvPrec5 99.750 | Reg term: 0.0\n",
      "Train Epoch:13 | Loss 0.5125 | AdvPrec1 79.188 | AdvPrec5 99.742 | Reg term: 0.0\n",
      "Train Epoch:14 | Loss 0.5042 | AdvPrec1 79.392 | AdvPrec5 99.792 | Reg term: 0.0\n",
      "Train Epoch:15 | Loss 0.4954 | AdvPrec1 79.728 | AdvPrec5 99.802 | Reg term: 0.0\n",
      "Val Epoch:15 | Loss 0.4203 | NatPrec1 83.570 | NatPrec5 99.880 | Reg term: 0.0 |\n",
      "Val Epoch:15 | Loss 0.6289 | AdvPrec1 74.700 | AdvPrec5 99.780 | Reg term: 0.0 |\n",
      "Train Epoch:16 | Loss 0.4885 | AdvPrec1 80.013 | AdvPrec5 99.822 | Reg term: 0.0\n",
      "Train Epoch:17 | Loss 0.4850 | AdvPrec1 80.185 | AdvPrec5 99.810 | Reg term: 0.0\n",
      "Train Epoch:18 | Loss 0.4816 | AdvPrec1 80.290 | AdvPrec5 99.833 | Reg term: 0.0\n",
      "Train Epoch:19 | Loss 0.4743 | AdvPrec1 80.638 | AdvPrec5 99.823 | Reg term: 0.0\n",
      "Train Epoch:20 | Loss 0.4695 | AdvPrec1 80.760 | AdvPrec5 99.835 | Reg term: 0.0\n",
      "Val Epoch:20 | Loss 0.3995 | NatPrec1 85.300 | NatPrec5 99.890 | Reg term: 0.0 |\n",
      "Val Epoch:20 | Loss 0.5963 | AdvPrec1 76.500 | AdvPrec5 99.750 | Reg term: 0.0 |\n",
      "Train Epoch:21 | Loss 0.4646 | AdvPrec1 80.985 | AdvPrec5 99.823 | Reg term: 0.0\n",
      "Train Epoch:22 | Loss 0.4627 | AdvPrec1 80.952 | AdvPrec5 99.855 | Reg term: 0.0\n",
      "Train Epoch:23 | Loss 0.4561 | AdvPrec1 81.175 | AdvPrec5 99.863 | Reg term: 0.0\n",
      "Train Epoch:24 | Loss 0.4524 | AdvPrec1 81.423 | AdvPrec5 99.872 | Reg term: 0.0\n",
      "Train Epoch:25 | Loss 0.4511 | AdvPrec1 81.433 | AdvPrec5 99.867 | Reg term: 0.0\n",
      "Val Epoch:25 | Loss 0.4137 | NatPrec1 83.640 | NatPrec5 99.880 | Reg term: 0.0 |\n",
      "Val Epoch:25 | Loss 0.6486 | AdvPrec1 74.190 | AdvPrec5 99.830 | Reg term: 0.0 |\n",
      "Train Epoch:26 | Loss 0.4469 | AdvPrec1 81.702 | AdvPrec5 99.885 | Reg term: 0.0\n",
      "Train Epoch:27 | Loss 0.4427 | AdvPrec1 81.658 | AdvPrec5 99.870 | Reg term: 0.0\n",
      "Train Epoch:28 | Loss 0.4379 | AdvPrec1 82.037 | AdvPrec5 99.875 | Reg term: 0.0\n",
      "Train Epoch:29 | Loss 0.4362 | AdvPrec1 82.098 | AdvPrec5 99.885 | Reg term: 0.0\n",
      "Train Epoch:30 | Loss 0.4338 | AdvPrec1 82.197 | AdvPrec5 99.903 | Reg term: 0.0\n",
      "Val Epoch:30 | Loss 0.3738 | NatPrec1 85.170 | NatPrec5 99.850 | Reg term: 0.0 |\n",
      "Val Epoch:30 | Loss 0.6128 | AdvPrec1 75.780 | AdvPrec5 99.710 | Reg term: 0.0 |\n",
      "Train Epoch:31 | Loss 0.4314 | AdvPrec1 82.238 | AdvPrec5 99.893 | Reg term: 0.0\n",
      "Train Epoch:32 | Loss 0.4294 | AdvPrec1 82.215 | AdvPrec5 99.910 | Reg term: 0.0\n",
      "Train Epoch:33 | Loss 0.4258 | AdvPrec1 82.562 | AdvPrec5 99.905 | Reg term: 0.0\n",
      "Train Epoch:34 | Loss 0.4247 | AdvPrec1 82.330 | AdvPrec5 99.895 | Reg term: 0.0\n",
      "Train Epoch:35 | Loss 0.4222 | AdvPrec1 82.687 | AdvPrec5 99.890 | Reg term: 0.0\n",
      "Val Epoch:35 | Loss 0.3581 | NatPrec1 86.080 | NatPrec5 99.910 | Reg term: 0.0 |\n",
      "Val Epoch:35 | Loss 0.6192 | AdvPrec1 76.450 | AdvPrec5 99.800 | Reg term: 0.0 |\n",
      "Train Epoch:36 | Loss 0.4203 | AdvPrec1 82.670 | AdvPrec5 99.890 | Reg term: 0.0\n",
      "Train Epoch:37 | Loss 0.4168 | AdvPrec1 82.670 | AdvPrec5 99.900 | Reg term: 0.0\n",
      "Train Epoch:38 | Loss 0.4154 | AdvPrec1 82.882 | AdvPrec5 99.925 | Reg term: 0.0\n",
      "Train Epoch:39 | Loss 0.4161 | AdvPrec1 82.883 | AdvPrec5 99.918 | Reg term: 0.0\n",
      "Train Epoch:40 | Loss 0.4163 | AdvPrec1 82.708 | AdvPrec5 99.910 | Reg term: 0.0\n",
      "Val Epoch:40 | Loss 0.3843 | NatPrec1 84.480 | NatPrec5 99.910 | Reg term: 0.0 |\n",
      "Val Epoch:40 | Loss 0.6395 | AdvPrec1 74.780 | AdvPrec5 99.800 | Reg term: 0.0 |\n",
      "Train Epoch:41 | Loss 0.4125 | AdvPrec1 82.850 | AdvPrec5 99.915 | Reg term: 0.0\n",
      "Train Epoch:42 | Loss 0.4090 | AdvPrec1 82.993 | AdvPrec5 99.933 | Reg term: 0.0\n",
      "Train Epoch:43 | Loss 0.4095 | AdvPrec1 83.025 | AdvPrec5 99.930 | Reg term: 0.0\n",
      "Train Epoch:44 | Loss 0.4081 | AdvPrec1 83.040 | AdvPrec5 99.938 | Reg term: 0.0\n",
      "Train Epoch:45 | Loss 0.4077 | AdvPrec1 83.055 | AdvPrec5 99.927 | Reg term: 0.0\n",
      "Val Epoch:45 | Loss 0.3979 | NatPrec1 84.550 | NatPrec5 99.870 | Reg term: 0.0 |\n",
      "Val Epoch:45 | Loss 0.6960 | AdvPrec1 74.020 | AdvPrec5 99.690 | Reg term: 0.0 |\n",
      "Train Epoch:46 | Loss 0.4044 | AdvPrec1 83.293 | AdvPrec5 99.937 | Reg term: 0.0\n",
      "Train Epoch:47 | Loss 0.4004 | AdvPrec1 83.328 | AdvPrec5 99.908 | Reg term: 0.0\n",
      "Train Epoch:48 | Loss 0.4006 | AdvPrec1 83.427 | AdvPrec5 99.938 | Reg term: 0.0\n",
      "Train Epoch:49 | Loss 0.3991 | AdvPrec1 83.330 | AdvPrec5 99.947 | Reg term: 0.0\n",
      "Train Epoch:50 | Loss 0.3546 | AdvPrec1 85.077 | AdvPrec5 99.958 | Reg term: 0.0\n",
      "Val Epoch:50 | Loss 0.3220 | NatPrec1 87.140 | NatPrec5 99.950 | Reg term: 0.0 |\n",
      "Val Epoch:50 | Loss 0.5854 | AdvPrec1 77.300 | AdvPrec5 99.830 | Reg term: 0.0 |\n",
      "Train Epoch:51 | Loss 0.3395 | AdvPrec1 85.827 | AdvPrec5 99.972 | Reg term: 0.0\n",
      "Train Epoch:52 | Loss 0.3363 | AdvPrec1 85.778 | AdvPrec5 99.978 | Reg term: 0.0\n",
      "Train Epoch:53 | Loss 0.3326 | AdvPrec1 85.947 | AdvPrec5 99.980 | Reg term: 0.0\n",
      "Train Epoch:54 | Loss 0.3287 | AdvPrec1 86.205 | AdvPrec5 99.978 | Reg term: 0.0\n",
      "Train Epoch:55 | Loss 0.3261 | AdvPrec1 86.175 | AdvPrec5 99.978 | Reg term: 0.0\n",
      "Val Epoch:55 | Loss 0.3273 | NatPrec1 86.890 | NatPrec5 99.950 | Reg term: 0.0 |\n",
      "Val Epoch:55 | Loss 0.6138 | AdvPrec1 76.500 | AdvPrec5 99.840 | Reg term: 0.0 |\n",
      "Train Epoch:56 | Loss 0.3252 | AdvPrec1 86.232 | AdvPrec5 99.985 | Reg term: 0.0\n",
      "Train Epoch:57 | Loss 0.3213 | AdvPrec1 86.475 | AdvPrec5 99.982 | Reg term: 0.0\n",
      "Train Epoch:58 | Loss 0.3188 | AdvPrec1 86.522 | AdvPrec5 99.987 | Reg term: 0.0\n",
      "Train Epoch:59 | Loss 0.3182 | AdvPrec1 86.458 | AdvPrec5 99.985 | Reg term: 0.0\n",
      "Train Epoch:60 | Loss 0.3157 | AdvPrec1 86.643 | AdvPrec5 99.987 | Reg term: 0.0\n",
      "Val Epoch:60 | Loss 0.3167 | NatPrec1 87.520 | NatPrec5 99.940 | Reg term: 0.0 |\n",
      "Val Epoch:60 | Loss 0.6043 | AdvPrec1 77.420 | AdvPrec5 99.860 | Reg term: 0.0 |\n",
      "Train Epoch:61 | Loss 0.3135 | AdvPrec1 86.702 | AdvPrec5 99.988 | Reg term: 0.0\n",
      "Train Epoch:62 | Loss 0.3136 | AdvPrec1 86.637 | AdvPrec5 99.990 | Reg term: 0.0\n",
      "Train Epoch:63 | Loss 0.3113 | AdvPrec1 86.815 | AdvPrec5 99.990 | Reg term: 0.0\n",
      "Train Epoch:64 | Loss 0.3090 | AdvPrec1 86.743 | AdvPrec5 99.993 | Reg term: 0.0\n",
      "Train Epoch:65 | Loss 0.3074 | AdvPrec1 86.937 | AdvPrec5 99.992 | Reg term: 0.0\n",
      "Val Epoch:65 | Loss 0.3353 | NatPrec1 86.970 | NatPrec5 99.930 | Reg term: 0.0 |\n",
      "Val Epoch:65 | Loss 0.6476 | AdvPrec1 76.240 | AdvPrec5 99.840 | Reg term: 0.0 |\n",
      "Train Epoch:66 | Loss 0.3068 | AdvPrec1 86.998 | AdvPrec5 99.993 | Reg term: 0.0\n",
      "Train Epoch:67 | Loss 0.3042 | AdvPrec1 87.052 | AdvPrec5 99.995 | Reg term: 0.0\n",
      "Train Epoch:68 | Loss 0.3033 | AdvPrec1 87.088 | AdvPrec5 99.993 | Reg term: 0.0\n",
      "Train Epoch:69 | Loss 0.3020 | AdvPrec1 87.122 | AdvPrec5 99.992 | Reg term: 0.0\n",
      "Train Epoch:70 | Loss 0.2996 | AdvPrec1 87.387 | AdvPrec5 99.990 | Reg term: 0.0\n",
      "Val Epoch:70 | Loss 0.3511 | NatPrec1 86.480 | NatPrec5 99.910 | Reg term: 0.0 |\n",
      "Val Epoch:70 | Loss 0.6852 | AdvPrec1 75.790 | AdvPrec5 99.790 | Reg term: 0.0 |\n",
      "Train Epoch:71 | Loss 0.2986 | AdvPrec1 87.212 | AdvPrec5 99.995 | Reg term: 0.0\n",
      "Train Epoch:72 | Loss 0.2959 | AdvPrec1 87.320 | AdvPrec5 99.992 | Reg term: 0.0\n",
      "Train Epoch:73 | Loss 0.2970 | AdvPrec1 87.377 | AdvPrec5 99.997 | Reg term: 0.0\n",
      "Train Epoch:74 | Loss 0.2954 | AdvPrec1 87.315 | AdvPrec5 99.992 | Reg term: 0.0\n",
      "Train Epoch:75 | Loss 0.2934 | AdvPrec1 87.422 | AdvPrec5 99.997 | Reg term: 0.0\n",
      "Val Epoch:75 | Loss 0.3237 | NatPrec1 87.640 | NatPrec5 99.940 | Reg term: 0.0 |\n",
      "Val Epoch:75 | Loss 0.6514 | AdvPrec1 76.440 | AdvPrec5 99.770 | Reg term: 0.0 |\n",
      "Train Epoch:76 | Loss 0.2924 | AdvPrec1 87.467 | AdvPrec5 99.995 | Reg term: 0.0\n",
      "Train Epoch:77 | Loss 0.2901 | AdvPrec1 87.575 | AdvPrec5 99.997 | Reg term: 0.0\n",
      "Train Epoch:78 | Loss 0.2884 | AdvPrec1 87.825 | AdvPrec5 99.993 | Reg term: 0.0\n",
      "Train Epoch:79 | Loss 0.2876 | AdvPrec1 87.600 | AdvPrec5 99.990 | Reg term: 0.0\n",
      "Train Epoch:80 | Loss 0.2871 | AdvPrec1 87.733 | AdvPrec5 99.995 | Reg term: 0.0\n",
      "Val Epoch:80 | Loss 0.3720 | NatPrec1 86.280 | NatPrec5 99.940 | Reg term: 0.0 |\n",
      "Val Epoch:80 | Loss 0.7474 | AdvPrec1 75.090 | AdvPrec5 99.750 | Reg term: 0.0 |\n",
      "Train Epoch:81 | Loss 0.2841 | AdvPrec1 87.862 | AdvPrec5 99.995 | Reg term: 0.0\n",
      "Train Epoch:82 | Loss 0.2838 | AdvPrec1 87.742 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:83 | Loss 0.2824 | AdvPrec1 87.957 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:84 | Loss 0.2802 | AdvPrec1 87.985 | AdvPrec5 99.995 | Reg term: 0.0\n",
      "Train Epoch:85 | Loss 0.2787 | AdvPrec1 88.123 | AdvPrec5 99.995 | Reg term: 0.0\n",
      "Val Epoch:85 | Loss 0.3394 | NatPrec1 87.480 | NatPrec5 99.940 | Reg term: 0.0 |\n",
      "Val Epoch:85 | Loss 0.7177 | AdvPrec1 76.380 | AdvPrec5 99.780 | Reg term: 0.0 |\n",
      "Train Epoch:86 | Loss 0.2770 | AdvPrec1 88.088 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:87 | Loss 0.2775 | AdvPrec1 88.030 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:88 | Loss 0.2755 | AdvPrec1 88.148 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:89 | Loss 0.2743 | AdvPrec1 88.367 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:90 | Loss 0.2717 | AdvPrec1 88.277 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Val Epoch:90 | Loss 0.3102 | NatPrec1 87.960 | NatPrec5 99.930 | Reg term: 0.0 |\n",
      "Val Epoch:90 | Loss 0.6838 | AdvPrec1 76.770 | AdvPrec5 99.710 | Reg term: 0.0 |\n",
      "Train Epoch:91 | Loss 0.2713 | AdvPrec1 88.407 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:92 | Loss 0.2701 | AdvPrec1 88.292 | AdvPrec5 99.993 | Reg term: 0.0\n",
      "Train Epoch:93 | Loss 0.2698 | AdvPrec1 88.423 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:94 | Loss 0.2659 | AdvPrec1 88.572 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:95 | Loss 0.2660 | AdvPrec1 88.520 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Val Epoch:95 | Loss 0.3399 | NatPrec1 87.090 | NatPrec5 99.940 | Reg term: 0.0 |\n",
      "Val Epoch:95 | Loss 0.7266 | AdvPrec1 75.630 | AdvPrec5 99.670 | Reg term: 0.0 |\n",
      "Train Epoch:96 | Loss 0.2643 | AdvPrec1 88.522 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:97 | Loss 0.2617 | AdvPrec1 88.657 | AdvPrec5 100.000 | Reg term: 0.\n",
      "Train Epoch:98 | Loss 0.2613 | AdvPrec1 88.663 | AdvPrec5 99.998 | Reg term: 0.0\n",
      "Train Epoch:99 | Loss 0.2610 | AdvPrec1 88.738 | AdvPrec5 99.997 | Reg term: 0.0\n",
      "Train Epoch:100 | Loss 0.2368 | AdvPrec1 89.943 | AdvPrec5 99.997 | Reg term: 0.\n",
      "Val Epoch:100 | Loss 0.3431 | NatPrec1 87.780 | NatPrec5 99.880 | Reg term: 0.0 \n",
      "Val Epoch:100 | Loss 0.7925 | AdvPrec1 75.820 | AdvPrec5 99.620 | Reg term: 0.0 \n",
      "Train Epoch:101 | Loss 0.2275 | AdvPrec1 90.282 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:102 | Loss 0.2227 | AdvPrec1 90.420 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:103 | Loss 0.2217 | AdvPrec1 90.428 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:104 | Loss 0.2187 | AdvPrec1 90.602 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:105 | Loss 0.2170 | AdvPrec1 90.715 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:105 | Loss 0.3490 | NatPrec1 87.970 | NatPrec5 99.910 | Reg term: 0.0 \n",
      "Val Epoch:105 | Loss 0.8203 | AdvPrec1 76.100 | AdvPrec5 99.520 | Reg term: 0.0 \n",
      "Train Epoch:106 | Loss 0.2158 | AdvPrec1 90.602 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:107 | Loss 0.2143 | AdvPrec1 90.725 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:108 | Loss 0.2120 | AdvPrec1 90.850 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:109 | Loss 0.2112 | AdvPrec1 90.838 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:110 | Loss 0.2102 | AdvPrec1 90.883 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:110 | Loss 0.3478 | NatPrec1 88.080 | NatPrec5 99.890 | Reg term: 0.0 \n",
      "Val Epoch:110 | Loss 0.8417 | AdvPrec1 76.270 | AdvPrec5 99.510 | Reg term: 0.0 \n",
      "Train Epoch:111 | Loss 0.2092 | AdvPrec1 90.973 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:112 | Loss 0.2077 | AdvPrec1 90.975 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:113 | Loss 0.2075 | AdvPrec1 91.062 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:114 | Loss 0.2042 | AdvPrec1 91.238 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:115 | Loss 0.2050 | AdvPrec1 91.052 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:115 | Loss 0.3509 | NatPrec1 88.150 | NatPrec5 99.870 | Reg term: 0.0 \n",
      "Val Epoch:115 | Loss 0.8612 | AdvPrec1 75.910 | AdvPrec5 99.590 | Reg term: 0.0 \n",
      "Train Epoch:116 | Loss 0.2033 | AdvPrec1 91.218 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:117 | Loss 0.2022 | AdvPrec1 91.302 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:118 | Loss 0.1996 | AdvPrec1 91.367 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:119 | Loss 0.1997 | AdvPrec1 91.298 | AdvPrec5 99.998 | Reg term: 0.\n",
      "Train Epoch:120 | Loss 0.1986 | AdvPrec1 91.470 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:120 | Loss 0.3663 | NatPrec1 87.930 | NatPrec5 99.880 | Reg term: 0.0 \n",
      "Val Epoch:120 | Loss 0.9071 | AdvPrec1 75.500 | AdvPrec5 99.540 | Reg term: 0.0 \n",
      "Train Epoch:121 | Loss 0.1968 | AdvPrec1 91.578 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:122 | Loss 0.1967 | AdvPrec1 91.423 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:123 | Loss 0.1961 | AdvPrec1 91.573 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:124 | Loss 0.1938 | AdvPrec1 91.622 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:125 | Loss 0.1930 | AdvPrec1 91.683 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:125 | Loss 0.3712 | NatPrec1 87.800 | NatPrec5 99.870 | Reg term: 0.0 \n",
      "Val Epoch:125 | Loss 0.9265 | AdvPrec1 75.270 | AdvPrec5 99.510 | Reg term: 0.0 \n",
      "Train Epoch:126 | Loss 0.1929 | AdvPrec1 91.635 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:127 | Loss 0.1901 | AdvPrec1 91.820 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:128 | Loss 0.1898 | AdvPrec1 91.888 | AdvPrec5 99.998 | Reg term: 0.\n",
      "Train Epoch:129 | Loss 0.1900 | AdvPrec1 91.912 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:130 | Loss 0.1871 | AdvPrec1 91.955 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:130 | Loss 0.3740 | NatPrec1 87.940 | NatPrec5 99.870 | Reg term: 0.0 \n",
      "Val Epoch:130 | Loss 0.9526 | AdvPrec1 75.080 | AdvPrec5 99.530 | Reg term: 0.0 \n",
      "Train Epoch:131 | Loss 0.1879 | AdvPrec1 92.013 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:132 | Loss 0.1869 | AdvPrec1 91.895 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:133 | Loss 0.1849 | AdvPrec1 91.967 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:134 | Loss 0.1842 | AdvPrec1 92.025 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:135 | Loss 0.1835 | AdvPrec1 92.140 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:135 | Loss 0.3967 | NatPrec1 87.660 | NatPrec5 99.870 | Reg term: 0.0 \n",
      "Val Epoch:135 | Loss 1.0226 | AdvPrec1 74.750 | AdvPrec5 99.420 | Reg term: 0.0 \n",
      "Train Epoch:136 | Loss 0.1818 | AdvPrec1 92.082 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:137 | Loss 0.1810 | AdvPrec1 92.257 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:138 | Loss 0.1814 | AdvPrec1 92.230 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:139 | Loss 0.1798 | AdvPrec1 92.295 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:140 | Loss 0.1774 | AdvPrec1 92.317 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:140 | Loss 0.3871 | NatPrec1 88.000 | NatPrec5 99.810 | Reg term: 0.0 \n",
      "Val Epoch:140 | Loss 0.9974 | AdvPrec1 75.070 | AdvPrec5 99.400 | Reg term: 0.0 \n",
      "Train Epoch:141 | Loss 0.1770 | AdvPrec1 92.333 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:142 | Loss 0.1767 | AdvPrec1 92.452 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:143 | Loss 0.1752 | AdvPrec1 92.447 | AdvPrec5 100.000 | Reg term: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:144 | Loss 0.1740 | AdvPrec1 92.490 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:145 | Loss 0.1731 | AdvPrec1 92.512 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:145 | Loss 0.3959 | NatPrec1 87.920 | NatPrec5 99.840 | Reg term: 0.0 \n",
      "Val Epoch:145 | Loss 1.0267 | AdvPrec1 74.950 | AdvPrec5 99.390 | Reg term: 0.0 \n",
      "Train Epoch:146 | Loss 0.1716 | AdvPrec1 92.688 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:147 | Loss 0.1713 | AdvPrec1 92.580 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:148 | Loss 0.1690 | AdvPrec1 92.720 | AdvPrec5 100.000 | Reg term: 0\n",
      "Train Epoch:149 | Loss 0.1683 | AdvPrec1 92.688 | AdvPrec5 100.000 | Reg term: 0\n",
      "Val Epoch:149 | Loss 0.4000 | NatPrec1 88.150 | NatPrec5 99.840 | Reg term: 0.0 \n",
      "Val Epoch:149 | Loss 1.0576 | AdvPrec1 74.920 | AdvPrec5 99.370 | Reg term: 0.0 \n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
